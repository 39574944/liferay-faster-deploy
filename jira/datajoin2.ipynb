{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 2: Combining LESA with JIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, our research question is as follows:\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of the LESA, LPS, LPP, and BPR ticket metadata that lets us group together different tickets so that we can explore the times that tickets remain in each status based on those groupings?</b>\n",
    "\n",
    "While this is inherently a data visualization question, what we can do is break it down into a smaller question and see if a little bit of data exploration gives us hints on how this data visualization should be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y beautifulsoup4 mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from checklpp import *\n",
    "\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we'll make sure that we establish one rule for this script and all future scripts: we will save all raw data with timestamps. This time, we're going to be saving the results of a database query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    user='lportal', password='lportal',\n",
    "    host='ec2-34-208-59-105.us-west-2.compute.amazonaws.com', database='lportal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_query(cache_name, query, row_function=None):\n",
    "    file_name = get_file_name(cache_name, '.json')\n",
    "    cursor.execute(query)\n",
    "\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        for i, item in enumerate(cursor):\n",
    "            if i % 1000 == 0:\n",
    "                print '[%s] Processed %d items' % (datetime.now().isoformat(), i)\n",
    "\n",
    "            row_value = {key: value for key, value in zip(cursor.column_names, item)}\n",
    "\n",
    "            if row_function is None:\n",
    "                save_row(outfile, [], row_value)\n",
    "                continue\n",
    "\n",
    "            for return_value in row_function(row_value):\n",
    "                save_row(outfile, [], return_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Explicit JIRA Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this assumes that you've loaded a backup of the LESA database from `files.liferay.com` into a database named `lportal`, because LESA (like many internal Liferay systems) lacks a useful API for data analysis, and therefore we will extract the data by querying the database directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from OSB_TicketLink where url like 'https://issues.liferay.com/%'\"\n",
    "\n",
    "save_query('JIRALink_1', query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Explicit JIRA Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some links are buried inside of the Liferay-only sections of comments and never formally linked on the ticket. Therefore, we'll need to perform some text extraction in order to identify those links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = re.compile('https://issues.liferay.com/browse/[A-Z]*-[\\d]*')\n",
    "\n",
    "patterns = [p1]\n",
    "\n",
    "def extract_links(row_value):\n",
    "    for url in [item for p in patterns for item in p.findall(row_value['body'])]:\n",
    "        yield {\n",
    "            'userName': row_value['userName'],\n",
    "            'url': url,\n",
    "            'createDate': row_value['createDate'],\n",
    "            'userId': row_value['userId'],\n",
    "            'visibility': row_value['visibility'],\n",
    "            'type_': row_value['type_'],\n",
    "            'ticketEntryId': row_value['ticketEntryId'],\n",
    "            'ticketCommentId': row_value['ticketCommentId']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"select * from OSB_TicketComment where body like '%https://issues.liferay.com/%'\"\n",
    "\n",
    "save_query('JIRALink_2', query, extract_links)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
