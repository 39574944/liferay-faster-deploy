{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 2: Visualizing JIRA Statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we start with the following research question. \"Can we create data visualizations on top of the LESA, LPS, LPP, and BPR ticket metadata that lets us group together different tickets so that we can explore the times that tickets remain in each status based on those groupings?\"\n",
    "\n",
    "In order to investigate the answer to this question, we start with a much smaller sub-question that focuses on more recent data.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "In order to answer this question, this notebook looks at simple visualizations of the JIRA data based on several ways you can group that data.\n",
    "\n",
    "At the end of this notebook, we will have a script that takes a sample of data from JIRA and enriches it with more data from JIRA, and the reader will have an improved understanding of the capabilities databases provide when it comes to processing information for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install matplotlib scikit-learn seaborn statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from checklpp import *\n",
    "from datetime import datetime, timedelta\n",
    "import functools\n",
    "import matplotlib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Process Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our computations can run independently of each other, so let's take advantage of some parallelization that's available on our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool = Pool(cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.4*2, 4.8*1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Database Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work at Liferay, it is easy to be lead to believe that databases exist as naive information storage and retrieval systems, because that is really all Liferay does with a database. However, the truth is much more complex.\n",
    "\n",
    "One of the other reasons databases exist is to manage relationships between data sets and allow you to analyze those relationships. As a result, many enterprise database vendors have created a rich set of proprietary functions on top of SQL that allow you to perform very insightful analysis.\n",
    "\n",
    "Because you're uncovering relationships within the data, whenever you want to answer questions, you often join together tables and then perform additional analysis on top of those joined tables. It is not uncommon for an analysis to simply take multiple sets of SQL used to generate join tables and use them as subqueries.\n",
    "\n",
    "Over time, it can be tedious to constantly paste in the the same nested queries over and over again, and having too many of them will also make it harder for anyone reading the query to understand what it is you were trying to analyze. To address this problem, it is common to create a database view.\n",
    "\n",
    "* [Why do you create a view in a database?](https://stackoverflow.com/questions/1278521/why-do-you-create-a-view-in-a-database)\n",
    "\n",
    "You can think of a database view as an alias for a query result that someone has needed before. We also know that every query result is a table, and knowing why people want that information to begin with (constantly reusing it in different analyses), you can also see why people might like a feature that allows you to materialize those views.\n",
    "\n",
    "* [Materialized view](https://en.wikipedia.org/wiki/Materialized_view)\n",
    "\n",
    "In our case, whenever we fetch tickets from JIRA, the JSON object may be cumbersome to work with. Therefore, in order to make it easier for people to understand the underlying information, we will create a view on top of it that allows us to concentrate on the specific fields we want to analyze. As we do this, however, we should keep in mind that all of the original raw data still exists, should we ever need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch LPP Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminding ourselves of our question:\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "The key part here is that when we retrieve each ticket from JIRA, we're interested in the times that tickets remain in each status. So, let's start by making sure that, at the very least, we can extract that metadata from an LPP ticket.\n",
    "\n",
    "First, we'll look at LPP tickets that relate to DXP, but are not any of the known workflow testing tickets (because those constantly close and re-open and might end up with the DXP affected version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpp_jql = \"\"\"\n",
    "project = LPP and affectedVersion = \"7.0 DE (7.0.10)\" and\n",
    "key not in (LPP-10825,LPP-10826,LPP-12114,LPP-13367)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that JIRA allows you to perform a join as part of its API by requesting an expansion of certain fields. Because we're interested in the time the ticket spends in each status, we can ask it to expand the `changelog` field, which effectively asks JIRA to perform a join on its equivalent of a `changelog` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    lpp_issues = get_jira_issues(lpp_jql, ['changelog'])\n",
    "else:\n",
    "    lpp_issues = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's accumulate the history transitions. Note that we're going to be making a simplification that will matter later on in the notebook: namely, we're going to assume that the creation of the issue is where we'll focus for the visualization, rather than the time of the transition.\n",
    "\n",
    "This can be a dangerous simplification if you are to use this for modeling or machine learning, but it's a useful simplification because it allows us to have a cumulative value for how long something spends in each status rather than having to understand each separate value (in the event that it enters the \"In Review\" status multiple times, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile jiratime.py\n",
    "from checklpp import get_time_delta_as_days\n",
    "from collections import defaultdict\n",
    "import dateparser\n",
    "from six import string_types\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "def extract_time(issue):\n",
    "    issue_created = dateparser.parse(issue['fields']['created']).date()\n",
    "\n",
    "    if region_field_name in issue['fields'] and issue['fields'][region_field_name] is not None:\n",
    "        regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "    else:\n",
    "        regions = ['']\n",
    "\n",
    "    old_status = 'Open'\n",
    "    current_assignee = issue['fields']['assignee']['displayName']\n",
    "    old_status_date = dateparser.parse(issue['fields']['created'])\n",
    "\n",
    "    status_times = defaultdict(lambda: defaultdict(float))\n",
    "    history_entries = issue['changelog']['histories']\n",
    "\n",
    "    for history_entry in history_entries:\n",
    "        history_entry['createdTime'] = dateparser.parse(history_entry['created'])\n",
    "\n",
    "    sorted_history_entries = sorted(\n",
    "        history_entries,\n",
    "        lambda x, y: -1 if get_time_delta_as_days(x['createdTime'] - y['createdTime']) < 0.0 else 1\n",
    "    )\n",
    "\n",
    "    for history_entry in sorted_history_entries:\n",
    "        useful_history_items = [\n",
    "            item for item in history_entry['items']\n",
    "                if item['field'] == 'status' or item['field'] == 'assignee'\n",
    "        ]\n",
    "\n",
    "        for item in useful_history_items:\n",
    "            if item['field'] == 'assignee':\n",
    "                current_assignee = item['fromString']\n",
    "            else:\n",
    "                old_status = item['fromString']\n",
    "\n",
    "            new_status_date = history_entry['createdTime']\n",
    "\n",
    "            elapsed_time = get_time_delta_as_days(new_status_date - old_status_date)\n",
    "\n",
    "            status_times[current_assignee][old_status] += elapsed_time\n",
    "            status_times['(all assignees)'][old_status] += elapsed_time\n",
    "\n",
    "            old_status_date = new_status_date\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            'jiraKey': issue['key'],\n",
    "            'type': issue['fields']['issuetype']['name'],\n",
    "            'assignee': assignee,\n",
    "            'region': regions[0],\n",
    "            'issueCreated': issue_created,\n",
    "            'status': status,\n",
    "            'elapsedTime': elapsed_time\n",
    "        }\n",
    "        for assignee, record in status_times.items()\n",
    "            for status, elapsed_time in record.items()\n",
    "    ]\n",
    "\n",
    "    return status_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block uses the `reload` ability to allow us to constantly change the time extraction above (which writes to a separate Python file so that it can be parallelized) and then reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiratime\n",
    "reload(jiratime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform our parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "num_finished = 0\n",
    "\n",
    "for result in pool.imap_unordered(jiratime.extract_time, lpp_issues.values()):\n",
    "    if num_finished % 100 == 0:\n",
    "        print('[%s] Processed %d of %d issues' % (datetime.now().isoformat(), num_finished, len(lpp_issues)))\n",
    "\n",
    "    num_finished += 1\n",
    "\n",
    "    for entry in result:\n",
    "        times.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our view of the JIRA data, naturally, we'll want to visualize it. We'll start with the most basic visualization: a table.\n",
    "\n",
    "When you start out in data visualization, the first thing you're taught is that when we generate descriptive statistics and visualize them as a table, we know that we're missing out on a large part of the story.\n",
    "\n",
    "This is because when you boil something down to a single number (whether it's a measure of central tendency or really any other descriptive statistics), all of the context is lost in that summarization. It's possible for things that have very similar statistics and even very similar linear regression predictions to actually be quite different from each other.\n",
    "\n",
    "* [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)\n",
    "\n",
    "However, tables give us a very concise overview of a lot of information at once, and they're extremely easy to generate, so they make for an excellent starting point to give you a sense of what you're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liferay has a lot of different statuses, so we'll limit our exploration to only a handful of states that we know are related to tickets that are in progress, and we'll ignore the individual assignee times (at least, in our initial analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statuses = ['Open', 'Verified', 'In Progress', 'In Review', 'On Hold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['status'].isin(statuses) & (df['assignee'] == '(all assignees)')]\n",
    "del df['assignee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we look at everything as a single unit, and we simply try to visualize how long we spend in each status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groupby = df[['status', 'elapsedTime']].groupby(['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_count = df_groupby.count()\n",
    "df_count = df_count.rename(columns={'elapsedTime': 'count'})\n",
    "df_norm1 = df_groupby.median()\n",
    "df_norm1 = df_norm1.rename(columns={'elapsedTime': 'elapsedTimeMedian'})\n",
    "df_norm2 = df_groupby.mean()\n",
    "df_norm2 = df_norm2.rename(columns={'elapsedTime': 'elapsedTimeMean'})\n",
    "df_quant = df_groupby.quantile([0.8, 0.9]).unstack()\n",
    "df_quant.columns = ['%s%d%%' % (col[0], col[1] * 100) for col in df_quant.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.join(df_norm1).join(df_norm2).join(df_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add one more level to this and group the statistics by region, and then take a look at what the statistics look like on the different ticket statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groupby = df[['region', 'status', 'elapsedTime']].groupby(['region', 'status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_count = df_groupby.count()\n",
    "df_count = df_count.rename(columns={'elapsedTime': 'count'})\n",
    "df_norm1 = df_groupby.median()\n",
    "df_norm1 = df_norm1.rename(columns={'elapsedTime': 'elapsedTimeMedian'})\n",
    "df_norm2 = df_groupby.mean()\n",
    "df_norm2 = df_norm2.rename(columns={'elapsedTime': 'elapsedTimeMean'})\n",
    "df_quant = df_groupby.quantile([0.8, 0.9]).unstack()\n",
    "df_quant.columns = ['%s%d%%' % (col[0], col[1] * 100) for col in df_quant.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.join(df_norm1).join(df_norm2).join(df_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plot Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to understand summary statistics is to visualize how the data is distributed. When looking at one-dimensional values (for example, the distribution of the time elapsed), this is often explained as a probability density function, which allows you to see how the data is concentrated. Peaks indicate that many values exist near that value.\n",
    "\n",
    "* [An introduction to kernel density estimation](http://www.mvstat.net/tduong/research/seminars/seminar-2001-05/)\n",
    "* [The idea of a probability density function](http://mathinsight.org/probability_density_function_idea)\n",
    "\n",
    "Once you have the probability density function, you can derive the cumulative distribution function. The values tell you how much of the data is less than a certain value, while the shape tells you how quickly that probability mass builds up.\n",
    "\n",
    "* [Cumulative distribution function](http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm)\n",
    "\n",
    "To plot these particular functions, we will be using the [seaborn](https://seaborn.pydata.org/) statistical data visualization library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df.groupby(['status']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], bw=0.5, label=key)\n",
    "\n",
    "    ax.set_xlim((0, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df.groupby(['status']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], cumulative=True, bw=0.5, label=key)\n",
    "\n",
    "    ax.set_xlim((0, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df[df['region'] == 'US'].groupby(['status']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], bw=0.5, label=key)\n",
    "\n",
    "    ax.set_xlim((0, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df[df['region'] == 'US'].groupby(['status']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], cumulative=True, bw=0.5, label=key)\n",
    "\n",
    "    ax.set_xlim((0, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can do is to use a sliding window over time as a way of estimating how long a ticket may take to close. In other words, for any given point in time, we take a look at all tickets opened thirty days before that point and see how long those tickets took to close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_rolling_plot(times, aggregator, status, region=None):\n",
    "    sorted_times = sorted(\n",
    "        [time for time in times if time['status'] == status and (region is None or time['region'] == region)],\n",
    "        lambda x, y: int(get_time_delta_as_days(x['issueCreated'] - y['issueCreated']))\n",
    "    )\n",
    "\n",
    "    if len(sorted_times) < 2:\n",
    "        return\n",
    "\n",
    "    start = 0\n",
    "    end = 1\n",
    "\n",
    "    max_end_date = sorted_times[end]['issueCreated']\n",
    "    min_start_date = max_end_date - timedelta(days=30)\n",
    "\n",
    "    rolling_window = [sorted_times[end]['elapsedTime']]\n",
    "\n",
    "    dates = []\n",
    "    values = []\n",
    "\n",
    "    while end + 1 < len(sorted_times):\n",
    "        while start + 1 < end and sorted_times[start+1]['issueCreated'] < min_start_date:\n",
    "            rolling_window.pop(0)\n",
    "            start += 1\n",
    "\n",
    "        while end < len(sorted_times) and sorted_times[end]['issueCreated'] < max_end_date:\n",
    "            rolling_window.append(sorted_times[end]['elapsedTime'])\n",
    "            end += 1\n",
    "\n",
    "        dates.append(min_start_date)\n",
    "        values.append(aggregator(rolling_window))\n",
    "\n",
    "        min_start_date += timedelta(days=1)\n",
    "        max_end_date = min(today + timedelta(days=1), max_end_date + timedelta(days=1))\n",
    "\n",
    "    plt.plot_date(dates, values, '-', label=status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sliding Window Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, np.median, status)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Sliding Window Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, np.median, status, 'US')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sliding Window Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, np.mean, status)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Region-Specific Sliding Window Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, np.mean, status, 'US')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Simple Sliding Window 90th Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, functools.partial(np.percentile, q=90), status)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Region-Specific Sliding Window 90th Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, functools.partial(np.percentile, q=90), status, 'US')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression as Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to look at the average difference introduced simply by changing a type. To do that, we could check how long each region spends in review as a table of linear regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_records(df, key_columns, value_column):\n",
    "    columns = key_columns + [value_column]\n",
    "\n",
    "    records = df[columns].to_dict(orient = 'records')\n",
    "\n",
    "    for record in records:\n",
    "        for key, value in record.items():\n",
    "            if value is None:\n",
    "                record[key] = ''\n",
    "\n",
    "    vectorizer = DictVectorizer()\n",
    "\n",
    "    train_x = vectorizer.fit_transform(\n",
    "        [{ key: value for key, value in record.items() if key != value_column } for record in records]\n",
    "    )\n",
    "\n",
    "    train_y = [record[value_column] for record in records]\n",
    "\n",
    "    return train_x, train_y, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
