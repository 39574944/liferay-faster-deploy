{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 2: Visualizing JIRA Statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we start with the following research question. \"Can we create data visualizations on top of the LESA, LPS, LPP, and BPR ticket metadata that lets us group together different tickets so that we can explore the times that tickets remain in each status based on those groupings?\"\n",
    "\n",
    "In order to investigate the answer to this question, we start with a much smaller sub-question that focuses on more recent data.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain in each status based on those groupings?</b>\n",
    "\n",
    "In order to answer this question, this notebook looks at simple visualizations of the JIRA data based on several ways you can group that data.\n",
    "\n",
    "At the end of this notebook, we will have a script that takes a sample of data from JIRA and enriches it with more data from JIRA, and the reader will have a further improved understanding of what goes into a join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install matplotlib scikit-learn seaborn statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from checklpp import *\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Process Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our computations can run independently of each other, so let's take advantage of some parallelization that's available on our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.4*2, 4.8*1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch JIRA Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminding ourselves of our question:\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS, LPP, and BPR ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain in each status based on those groupings?</b>\n",
    "\n",
    "The key part here is that when we retrieve each ticket from JIRA, we're interested in the times that tickets remain in each status. So, let's start by making sure that, at the very least, we can extract that metadata from an LPP ticket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch LPP Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at LPP tickets that relate to DXP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpp_jql = 'project = LPP and affectedVersion = \"7.0 DE (7.0.10)\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that JIRA allows you to perform a join as part of its API by requesting an expansion of certain fields. Because we're interested in the time the ticket spends in each status, we can ask it to expand the `changelog` field, which effectively asks JIRA to perform a join on its equivalent of a `changelog` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    lpp_issues = get_jira_issues(lpp_jql, ['changelog'])\n",
    "else:\n",
    "    lpp_issues = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's accumulate the history transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile jiratime.py\n",
    "from checklpp import get_time_delta_as_days\n",
    "from collections import defaultdict\n",
    "import dateparser\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "def extract_time(issue):\n",
    "    status_times = defaultdict(float)\n",
    "    old_status_date = dateparser.parse(issue['fields']['created'])\n",
    "\n",
    "    for history_entry in issue['changelog']['histories']:\n",
    "        status_history = [item for item in history_entry['items'] if item['field'] == 'status']\n",
    "\n",
    "        for item in status_history:\n",
    "            old_status = item['fromString']\n",
    "            new_status_date = dateparser.parse(history_entry['created'])\n",
    "\n",
    "            elapsed_time = get_time_delta_as_days(new_status_date - old_status_date)\n",
    "\n",
    "            status_times[old_status] += elapsed_time\n",
    "\n",
    "            old_status_date = new_status_date\n",
    "\n",
    "    if region_field_name in issue['fields'] and issue['fields'][region_field_name] is not None:\n",
    "        regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "    else:\n",
    "        regions = ['']\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            'jiraKey': issue['key'],\n",
    "            'type': issue['fields']['issuetype']['name'],\n",
    "            'region': regions[0],\n",
    "            'issueCreated': dateparser.parse(issue['fields']['created']).date(),\n",
    "            'status': status,\n",
    "            'elapsedTime': elapsed_time\n",
    "        }\n",
    "        for status, elapsed_time in status_times.items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiratime\n",
    "reload(jiratime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "num_finished = 0\n",
    "\n",
    "for result in pool.imap_unordered(jiratime.extract_time, lpp_issues.values()):\n",
    "    if num_finished % 100 == 0:\n",
    "        print('[%s] Processed %d of %d issues' % (datetime.now().isoformat(), num_finished, len(lpp_issues)))\n",
    "\n",
    "    num_finished += 1\n",
    "        \n",
    "    for entry in result:        \n",
    "        times.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statuses = ['Open', 'Verified', 'In Progress', 'In Review', 'On Hold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['status'].isin(statuses)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with some basic statistics by status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groupby = df[['status', 'elapsedTime']].groupby(['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_count = df_groupby.count()\n",
    "df_count = df_count.rename(columns={'elapsedTime': 'count'})\n",
    "df_norm1 = df_groupby.median()\n",
    "df_norm1 = df_norm1.rename(columns={'elapsedTime': 'elapsedTimeMedian'})\n",
    "df_norm2 = df_groupby.mean()\n",
    "df_norm2 = df_norm2.rename(columns={'elapsedTime': 'elapsedTimeMean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.join(df_norm1).join(df_norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add one more level of this and show the raw statistics by region and status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groupby = df[['region', 'status', 'elapsedTime']].groupby(['region', 'status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_count = df_groupby.count()\n",
    "df_count = df_count.rename(columns={'elapsedTime': 'count'})\n",
    "df_norm1 = df_groupby.median()\n",
    "df_norm1 = df_norm1.rename(columns={'elapsedTime': 'elapsedTimeMedian'})\n",
    "df_norm2 = df_groupby.mean()\n",
    "df_norm2 = df_norm2.rename(columns={'elapsedTime': 'elapsedTimeMean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.join(df_norm1).join(df_norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plot Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df.groupby(['status']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], bw=0.5, label=key)\n",
    "    \n",
    "    ax.set_xlim((0, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df[df['region'] == 'US'].groupby(['status']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], bw=0.5, label=key)\n",
    "    \n",
    "    ax.set_xlim((0, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can do is to use a sliding window over time as a way of estimating how long a ticket may take to close. In other words, for any given point in time, we take a look at all tickets opened thirty days before that point and see how long those tickets took to close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_rolling_plot(times, aggregator, status, region=None):\n",
    "    sorted_times = sorted(\n",
    "        [time for time in times if time['status'] == status and (region is None or time['region'] == region)],\n",
    "        lambda x, y: int(get_time_delta_as_days(x['issueCreated'] - y['issueCreated']))\n",
    "    )\n",
    "    \n",
    "    if len(sorted_times) < 2:\n",
    "        return\n",
    "    \n",
    "    start = 0\n",
    "    end = 1\n",
    "    \n",
    "    max_end_date = sorted_times[end]['issueCreated']\n",
    "    min_start_date = max_end_date - timedelta(days=30)\n",
    "\n",
    "    rolling_window = [sorted_times[end]['elapsedTime']]\n",
    "\n",
    "    dates = []\n",
    "    values = []\n",
    "\n",
    "    while end + 1 < len(sorted_times):\n",
    "        while start + 1 < end and sorted_times[start+1]['issueCreated'] < min_start_date:\n",
    "            rolling_window.pop(0)\n",
    "            start += 1\n",
    "\n",
    "        while end < len(sorted_times) and sorted_times[end]['issueCreated'] < max_end_date:\n",
    "            rolling_window.append(sorted_times[end]['elapsedTime'])\n",
    "            end += 1\n",
    "\n",
    "        dates.append(min_start_date)\n",
    "        values.append(aggregator(rolling_window))\n",
    "\n",
    "        min_start_date += timedelta(days=1)\n",
    "        max_end_date = min(today + timedelta(days=1), max_end_date + timedelta(days=1))\n",
    "    \n",
    "    plt.plot_date(dates, values, '-', label=status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sliding Window Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, np.median, status)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Sliding Window Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, np.median, status, 'US')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sliding Window Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, np.mean, status)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Region-Specific Sliding Window Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in statuses:\n",
    "    get_time_rolling_plot(times, np.mean, status, 'US')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression as Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to look at the average difference introduced simply by changing a type. To do that, we could check how long each region spends in review as a table of linear regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_records(df, key_columns, value_column):\n",
    "    columns = key_columns + [value_column]\n",
    "\n",
    "    records = df[columns].to_dict(orient = 'records')\n",
    "\n",
    "    for record in records:\n",
    "        for key, value in record.items():\n",
    "            if value is None:\n",
    "                record[key] = ''\n",
    "\n",
    "    vectorizer = DictVectorizer()\n",
    "\n",
    "    train_x = vectorizer.fit_transform(\n",
    "        [{ key: value for key, value in record.items() if key != value_column } for record in records]\n",
    "    )\n",
    "    \n",
    "    train_y = [record[value_column] for record in records]\n",
    "\n",
    "    return train_x, train_y, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
