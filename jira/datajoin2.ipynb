{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 2: Visualizing JIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we start with the following research question. \"Can we create data visualizations on top of the LESA, LPS, LPP, and BPR ticket metadata that lets us group together different tickets so that we can explore the times that tickets remain in each status based on those groupings?\"\n",
    "\n",
    "In order to investigate the answer to this question, we start with a much smaller sub-question that focuses on more recent data.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain in each status based on those groupings?</b>\n",
    "\n",
    "In order to answer this question, this notebook looks at simple visualizations of the JIRA data based on several ways you can group that data.\n",
    "\n",
    "At the end of this notebook, we will have a script that takes a sample of data from JIRA and enriches it with more data from JIRA, and the reader will have a further improved understanding of what goes into a join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install matplotlib scikit-learn statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from checklpp import *\n",
    "import dateparser\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch JIRA Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminding ourselves of our question:\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS, LPP, and BPR ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain in each status based on those groupings?</b>\n",
    "\n",
    "The key part here is that when we retrieve each ticket from JIRA, we're interested in the times that tickets remain in each status. So, let's start by making sure that, at the very least, we can extract that metadata from an LPP ticket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch LPP Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at LPP tickets that relate to DXP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpp_jql = 'project = LPP and affectedVersion = \"7.0 DE (7.0.10)\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that JIRA allows you to perform a join as part of its API by requesting an expansion of certain fields. Because we're interested in the time the ticket spends in each status, we can ask it to expand the `changelog` field, which effectively asks JIRA to perform a join on its equivalent of a `changelog` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    lpp_issues = get_jira_issues(lpp_jql, ['changelog'])\n",
    "else:\n",
    "    lpp_issues = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's accumulate the history transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "for num, issue in enumerate(lpp_issues.values()):\n",
    "    if num % 100 == 0:\n",
    "        print('[%s] Processed %d of %d issues' % (datetime.now().isoformat(), num, len(lpp_issues)))\n",
    "    \n",
    "    status_times = defaultdict(float)\n",
    "    old_status_date = dateparser.parse(issue['fields']['created'])\n",
    "\n",
    "    for history_entry in issue['changelog']['histories']:\n",
    "        status_history = [item for item in history_entry['items'] if item['field'] == 'status']\n",
    "\n",
    "        for item in status_history:\n",
    "            old_status = item['fromString']\n",
    "            new_status_date = dateparser.parse(history_entry['created'])\n",
    "\n",
    "            elapsed_time = get_time_delta_as_days(new_status_date - old_status_date)\n",
    "\n",
    "            status_times[old_status] += elapsed_time\n",
    "\n",
    "            old_status_date = new_status_date\n",
    "\n",
    "    if region_field_name in issue['fields'] and issue['fields'][region_field_name] is not None:\n",
    "        regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "    else:\n",
    "        regions = ['']\n",
    "            \n",
    "    for status, elapsed_time in status_times.items():\n",
    "        entry = {\n",
    "            'jiraKey': issue['key'],\n",
    "            'type': issue['fields']['issuetype']['name'],\n",
    "            'region': regions[0],\n",
    "            'issueCreated': issue['fields']['created'],\n",
    "            'status': status,\n",
    "            'elapsedTime': elapsed_time\n",
    "        }\n",
    "        \n",
    "        times.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize LPP Ticket Status Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest visualization is to show how often each region uses each status as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['region', 'status', 'elapsedTime']].groupby(['region', 'status']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also see what the average time is for each status for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['region', 'status', 'elapsedTime']].groupby(['region', 'status']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to look at differences between regions. For example, we could check how long each region spends in review as a table of linear regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_records(df, key_columns, value_column):\n",
    "    columns = key_columns + [value_column]\n",
    "\n",
    "    records = df[columns].to_dict(orient = 'records')\n",
    "\n",
    "    for record in records:\n",
    "        for key, value in record.items():\n",
    "            if value is None:\n",
    "                record[key] = ''\n",
    "\n",
    "    vectorizer = DictVectorizer()\n",
    "\n",
    "    train_x = vectorizer.fit_transform(\n",
    "        [{ key: value for key, value in record.items() if key != value_column } for record in records]\n",
    "    )\n",
    "    \n",
    "    train_y = [record[value_column] for record in records]\n",
    "\n",
    "    return train_x, train_y, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
