{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 2: Combining LESA with JIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we start with the following research question. \"Can we create data visualizations on top of the LESA, LPS, LPP, and BPR ticket metadata that lets us group together different tickets so that we can explore the times that tickets remain in each status based on those groupings?\"\n",
    "\n",
    "While this is inherently a data visualization question, what we can do is convert it into a smaller question and see if a little bit of data exploration gives us hints on how this data visualization should be implemented.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LESA, LPS, LPP, and BPR ticket metadata that lets us group together DXP tickets so that we can explore the times that tickets remain in each status based on those groupings?</b>\n",
    "\n",
    "This notebook assumes that you've loaded a backup of the LESA database from `files.liferay.com` into a database named `lportal`, because LESA (like many internal Liferay systems) lacks a useful API for data analysis, and therefore we will extract the data by querying the database directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y beautifulsoup4 mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from checklpp import *\n",
    "\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we'll make sure that we establish one rule for this script and all future scripts: we will save all raw data with timestamps. This time, we're going to be saving the results of a database query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    user='lportal', password='lportal',\n",
    "    host='ec2-34-208-59-105.us-west-2.compute.amazonaws.com', database='lportal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_query(cache_name, query, row_function=None):\n",
    "    file_name = get_file_name(cache_name, 'json')\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        print('[%s] Skipping query execution due to cache file' % datetime.now().isoformat())\n",
    "        return\n",
    "\n",
    "    print('[%s] Executing query %s' % (datetime.now().isoformat(), query))\n",
    "    \n",
    "    cursor.execute(query)\n",
    "\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        for i, item in enumerate(cursor):\n",
    "            if i % 1000 == 0:\n",
    "                print('[%s] Processed %d items' % (datetime.now().isoformat(), i))\n",
    "\n",
    "            row_value = {key: value for key, value in zip(cursor.column_names, item)}\n",
    "\n",
    "            if row_function is None:\n",
    "                save_row(outfile, [], row_value)\n",
    "                continue\n",
    "\n",
    "            for return_value in row_function(row_value):\n",
    "                save_row(outfile, [], return_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_query(cache_name, row_function=None):\n",
    "    file_name = get_file_name(cache_name, 'json')\n",
    "\n",
    "    with open(file_name, 'r') as infile:\n",
    "        if row_function is None:\n",
    "            return [load_row(line) for line in infile]\n",
    "        else:\n",
    "            return [row_function(load_row(line)) for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explain_query(query):\n",
    "    print('[%s] Explaining query %s' % (datetime.now().isoformat(), query))\n",
    "    \n",
    "    cursor.execute('explain %s' % query)\n",
    "\n",
    "    return [\n",
    "        { key: value for key, value in zip(cursor.column_names, item) }\n",
    "            for item in cursor\n",
    "    ]\n",
    "\n",
    "def run_query(query):\n",
    "    print('[%s] Executing query %s' % (datetime.now().isoformat(), query))\n",
    "    \n",
    "    cursor.execute(query)\n",
    "\n",
    "    return [\n",
    "        { key: value for key, value in zip(cursor.column_names, item) }\n",
    "            for item in cursor\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract JIRA Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Explicit JIRA Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some links are found in the `OSB_TicketLink` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from OSB_TicketLink where url like 'https://issues.liferay.com/%'\n",
    "\"\"\"\n",
    "\n",
    "save_query('lesa/JIRALink_1', query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract JIRA Links in Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some links are buried inside of the Liferay-only sections of comments and never formally linked on the ticket. Therefore, we'll need to perform some text extraction in order to identify those links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = re.compile('https://issues.liferay.com/browse/[A-Z]*-[\\d]*')\n",
    "\n",
    "patterns = [p1]\n",
    "\n",
    "def extract_links(row_value):\n",
    "    for url in [item for p in patterns for item in p.findall(row_value['body'])]:\n",
    "        yield {\n",
    "            'userName': row_value['userName'],\n",
    "            'url': url,\n",
    "            'createDate': row_value['createDate'],\n",
    "            'userId': row_value['userId'],\n",
    "            'visibility': row_value['visibility'],\n",
    "            'type_': row_value['type_'],\n",
    "            'ticketEntryId': row_value['ticketEntryId'],\n",
    "            'ticketCommentId': row_value['ticketCommentId']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from OSB_TicketComment where body like '%https://issues.liferay.com/%'\n",
    "\"\"\"\n",
    "\n",
    "save_query('lesa/JIRALink_2', query, extract_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Database Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Liferay Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each LESA ticket found in the `OSB_TicketEntry` table, we identify the version of Liferay based on the value in the `envLFR` column. Each value in this column corresponds to an entry in the `ListType` table, with a few different types, all of them having a `type_` that starts with `com.liferay.osb.model.ProductEntry`.\n",
    "\n",
    "Of course, not all of these are used.\n",
    "\n",
    "First, we'll find the used `ListType` values that correspond to Liferay versions, which can be achieved by looking for the distinct values of the `envLFR` table and checking for the matching `listTypeId` value in the `ListType` table. The query might look like the following.\n",
    "\n",
    "```\n",
    "SELECT * \n",
    "FROM   ListType \n",
    "WHERE  listTypeId IN (SELECT DISTINCT envLFR \n",
    "                      FROM   OSB_TicketEntry) \n",
    "       AND type_ LIKE 'com.liferay.osb.model.ProductEntry.%' \n",
    "```\n",
    "\n",
    "However, before we do that, we'll need to see if it's a good idea to execute the query by looking at the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guess the Query Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the database indices and compare them to our database query to see if we have an intuition about how the database query will take shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(run_query('show indexes from ListType'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(run_query('show indexes from OSB_TicketEntry'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Query Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM   ListType \n",
    "WHERE  listTypeId IN (SELECT DISTINCT envLFR \n",
    "                      FROM   OSB_TicketEntry) \n",
    "       AND type_ LIKE 'com.liferay.osb.model.ProductEntry.%'\n",
    "\"\"\"\n",
    "\n",
    "pd.DataFrame(explain_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've never seen this format before, the MySQL documentation provides a good high-level explanation of what is going on in the above query.\n",
    "\n",
    "* [Explain Output Format](https://dev.mysql.com/doc/refman/5.7/en/explain-output.html#explain-join-types)\n",
    "\n",
    "According to the explain plan, the database will materialize our subquery.\n",
    "\n",
    "`select envLFR from OSB_TicketEntry`\n",
    "\n",
    "From there, it will compute the distinct values. Most databases compute a distinct by performing a sort and then iterating over the sorted result so that it can pick off the distinct values.\n",
    "\n",
    "This list of distinct values will then be used with a filtered `ListType` table to find the entries of interest. Because we have only a prefix on our `LIKE` clause, and because there is an index on the `type_` column, this filtering is implemented as a range query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pd.DataFrame(run_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_query('lesa/ListType_EnvLFR', query)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
