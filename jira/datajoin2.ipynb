{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 2: Visualizing JIRA Statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we start with the following research question. \"Can we create data visualizations on top of the LESA, LPS, LPP, and BPR ticket metadata that lets us group together different tickets so that we can explore the times that tickets remain in each status based on those groupings?\"\n",
    "\n",
    "In order to investigate the answer to this question, we start with a much smaller sub-question that focuses on more recent data.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "In order to answer this question, this notebook looks at simple visualizations of the JIRA data based on several ways you can group that data.\n",
    "\n",
    "At the end of this notebook, we will have a script that takes a sample of data from JIRA and enriches it with more data from JIRA, and the reader will have an improved understanding of the capabilities databases provide when it comes to processing information for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!conda install matplotlib scikit-learn seaborn statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from checklpp import *\n",
    "from datetime import datetime, timedelta\n",
    "import functools\n",
    "import matplotlib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Process Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our computations can run independently of each other, so let's take advantage of some parallelization that's available on our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool = Pool(cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also change some of the default plots so that they're larger and use a white background instead of a gray one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.4*2, 4.8*1.5)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also change some of our default colors to make them a little more color-blind friendly than the default plotting colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('colorblind')\n",
    "sns.palplot(sns.color_palette('colorblind'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Database Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work at Liferay, it is easy to be lead to believe that databases exist as naive information storage and retrieval systems, because that is really all Liferay does with a database. However, the truth is much more complex.\n",
    "\n",
    "One of the other reasons databases exist is to manage relationships between data sets and allow you to analyze those relationships. As a result, many enterprise database vendors have created a rich set of proprietary functions on top of SQL that allow you to perform very insightful analysis.\n",
    "\n",
    "Because you're uncovering relationships within the data, whenever you want to answer questions, you often join together tables and then perform additional analysis on top of those joined tables. It is not uncommon for an analysis to simply take multiple sets of SQL used to generate join tables and use them as subqueries.\n",
    "\n",
    "Over time, it can be tedious to constantly paste in the the same nested queries over and over again, and having too many of them will also make it harder for anyone reading the query to understand what it is you were trying to analyze. To address this problem, it is common to create a database view.\n",
    "\n",
    "* [Why do you create a view in a database?](https://stackoverflow.com/questions/1278521/why-do-you-create-a-view-in-a-database)\n",
    "\n",
    "You can think of a database view as an alias for a query result that someone has needed before. We also know that every query result is a table, and knowing why people want that information to begin with (constantly reusing it in different analyses), you can also see why people might like a feature that allows you to materialize those views.\n",
    "\n",
    "* [Materialized view](https://en.wikipedia.org/wiki/Materialized_view)\n",
    "\n",
    "In our case, whenever we fetch tickets from JIRA, the JSON object may be cumbersome to work with. Therefore, in order to make it easier for people to understand the underlying information, we will create a view on top of it that allows us to concentrate on the specific fields we want to analyze. As we do this, however, we should keep in mind that all of the original raw data still exists, should we ever need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch LPP Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminding ourselves of our question:\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "The key part here is that when we retrieve each ticket from JIRA, we're interested in the times that tickets remain in each status. So, let's start by making sure that, at the very least, we can extract that metadata from an LPP ticket.\n",
    "\n",
    "First, we'll look at LPP tickets that relate to DXP, but are not any of the known workflow testing tickets (because those constantly close and re-open and might end up with the DXP affected version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpp_jql = \"\"\"\n",
    "project = LPP and affectedVersion = \"7.0 DE (7.0.10)\" and\n",
    "key not in (LPP-10825,LPP-10826,LPP-12114,LPP-13367)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that JIRA allows you to perform a join as part of its API by requesting an expansion of certain fields. Because we're interested in the time the ticket spends in each status, we can ask it to expand the `changelog` field, which effectively asks JIRA to perform a join on its equivalent of a `changelog` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    lpp_issues = get_jira_issues(lpp_jql, ['changelog'])\n",
    "else:\n",
    "    lpp_issues = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's accumulate the history transitions. Note that we're going to be making a simplification that will matter later on in the notebook: namely, we're going to assume that the creation of the issue is where we'll focus for the visualization, rather than the time of the transition.\n",
    "\n",
    "This can be a dangerous simplification if you are to use this for modeling or machine learning, but it's a useful simplification because it allows us to have a cumulative value for how long something spends in each status rather than having to understand each separate value (in the event that it enters the \"In Review\" status multiple times, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile jiratime.py\n",
    "from checklpp import get_time_delta_as_days\n",
    "from collections import defaultdict\n",
    "import dateparser\n",
    "from six import string_types\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "def extract_time(issue):\n",
    "    issue_created = dateparser.parse(issue['fields']['created']).date()\n",
    "\n",
    "    if region_field_name in issue['fields'] and issue['fields'][region_field_name] is not None:\n",
    "        regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "    else:\n",
    "        regions = ['']\n",
    "\n",
    "    old_status = 'Open'\n",
    "    current_assignee = issue['fields']['assignee']['displayName']\n",
    "    old_status_date = dateparser.parse(issue['fields']['created'])\n",
    "\n",
    "    status_times = defaultdict(lambda: defaultdict(float))\n",
    "    history_entries = issue['changelog']['histories']\n",
    "\n",
    "    for history_entry in history_entries:\n",
    "        history_entry['createdTime'] = dateparser.parse(history_entry['created'])\n",
    "\n",
    "    sorted_history_entries = sorted(\n",
    "        history_entries,\n",
    "        lambda x, y: -1 if get_time_delta_as_days(x['createdTime'] - y['createdTime']) < 0.0 else 1\n",
    "    )\n",
    "\n",
    "    for history_entry in sorted_history_entries:\n",
    "        useful_history_items = [\n",
    "            item for item in history_entry['items']\n",
    "                if item['field'] == 'status' or item['field'] == 'assignee'\n",
    "        ]\n",
    "\n",
    "        for item in useful_history_items:\n",
    "            if item['field'] == 'assignee':\n",
    "                current_assignee = item['fromString']\n",
    "            else:\n",
    "                old_status = item['fromString']\n",
    "\n",
    "            new_status_date = history_entry['createdTime']\n",
    "\n",
    "            elapsed_time = get_time_delta_as_days(new_status_date - old_status_date)\n",
    "\n",
    "            status_times[current_assignee][old_status] += elapsed_time\n",
    "            status_times['(all assignees)'][old_status] += elapsed_time\n",
    "\n",
    "            old_status_date = new_status_date\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            'jiraKey': issue['key'],\n",
    "            'type': issue['fields']['issuetype']['name'],\n",
    "            'assignee': assignee,\n",
    "            'region': regions[0],\n",
    "            'issueCreated': issue_created,\n",
    "            'status': status,\n",
    "            'elapsedTime': elapsed_time\n",
    "        }\n",
    "        for assignee, record in status_times.items()\n",
    "            for status, elapsed_time in record.items()\n",
    "    ]\n",
    "\n",
    "    return status_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block uses the `reload` ability to allow us to constantly change the time extraction above (which writes to a separate Python file so that it can be parallelized) and then reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiratime\n",
    "reload(jiratime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform our parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "num_finished = 0\n",
    "\n",
    "for result in pool.imap_unordered(jiratime.extract_time, lpp_issues.values()):\n",
    "    if num_finished % 100 == 0:\n",
    "        print('[%s] Processed %d of %d issues' % (datetime.now().isoformat(), num_finished, len(lpp_issues)))\n",
    "\n",
    "    num_finished += 1\n",
    "\n",
    "    for entry in result:\n",
    "        times.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our view of the JIRA data, let's revisit our research question.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "Given our question, naturally, our next step is to visualize our JIRA data. We'll start with the most basic visualization: a table.\n",
    "\n",
    "When you start out in data visualization, the first thing you're taught is that when we generate descriptive statistics and visualize them as a table, we know that we're missing out on a large part of the story.\n",
    "\n",
    "This is because when you boil something down to a single number (whether it's a measure of central tendency or really any other descriptive statistics), all of the context is lost in that summarization. It's possible for things that have very similar statistics and even very similar linear regression predictions to actually be quite different from each other.\n",
    "\n",
    "* [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)\n",
    "\n",
    "However, tables give us a very concise overview of a lot of information at once, and they're extremely easy to generate, so they make for an excellent starting point to give you a sense of what you're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_times = [entry for entry in times if entry['assignee'] == '(all assignees)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(aggregate_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liferay has a lot of different statuses, so we'll limit our exploration to only a handful of states that we know are related to tickets that are in progress, and we'll ignore the individual assignee times (at least, in our initial analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statuses = ['Open', 'Verified', 'In Progress', 'In Review', 'On Hold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['status'].isin(statuses)]\n",
    "del df['assignee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Tabulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin creating tables that summarize the table across a numerical statistic (in our case, elapsed time), it's useful to do some data validation so that we can check our assumptions.\n",
    "\n",
    "One validation we might do is this: how much each status is used for each different ticket type in our data set? This question can be answered with a contingency table, or cross-tabulation, which is available in almost all statistic computing libraries, because of how simple yet useful they are as a visualization tool.\n",
    "\n",
    "* [Crosstabs](http://libguides.library.kent.edu/SPSS/Crosstabs)\n",
    "\n",
    "A cross-tabulation looks at how two or more columns co-occur with each other, and it's especially useful when examining how two categorical variables relate to each other.\n",
    "\n",
    "* [Categorical Variable](https://en.wikipedia.org/wiki/Categorical_variable)\n",
    "\n",
    "This allows you to check any assumptions about the data, such as whether two column values never co-occur, or whether two column values seem to co-occur a lot more than expected. This kind of visualization is also relevant for our original question, where we will look only at \"In Review\" statuses, where we might want to make sure that any assumptions about \"In Review\" are accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['status'], [df['type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder, can you cross-tabulate two non-categorical columns, such as two columns that are both continuous, floating point values?\n",
    "\n",
    "* [Continuous and Discrete Variable](https://en.wikipedia.org/wiki/Continuous_and_discrete_variable)\n",
    "\n",
    "If you think about it, a cross-tabulation will have too many columns and too many rows. In this case, if you are prefer to use a table to start out, one popular option is to simply convert the continuous variable into a discrete variable using binning, which allows you to tabulate the resulting artificial categories.\n",
    "\n",
    "* [Data binning](https://en.wikipedia.org/wiki/Data_binning)\n",
    "\n",
    "For the more mathematics and visually oriented, this table might eliminate too much information if you do not choose your discretization well. An alternative popular option is to simply interpret the co-occurrences as probabilities rather than raw counts, which allows you to construct a contour plot of the the joint probability function.\n",
    "\n",
    "If you've never heard of a contour plot before, imagine that one variable is geo latitude, and another variable is geo longitude. A famous contour plot would be something that depicts the elevation at each geo latitude. The end result is an elevation map, which you can render in two dimensions with color, or allow interacting in three dimensions like in Google Earth.\n",
    "\n",
    "* [Contour plots](http://www.statisticshowto.com/contour-plots/)\n",
    "\n",
    "If you don't remember what a joint probability function is, and you're a visual learner, these lecture notes give a good visual refresher, and also provide visualizations that help you connect the idea behind a joint probability distribution to contour plots.\n",
    "\n",
    "* [Joint Density Functions, Marginal Density Functions, Conditional Density Functions, Expectations and Independence](http://www.colorado.edu/economics/morey/7818/jointdensity/jointdensity.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we look at everything as a single unit, and we simply try to visualize how long we spend in each status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groupby = df[['status', 'elapsedTime']].groupby(['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common statistics that people like to look at include measures of central tendency (mean, median), as well as quantiles (x% of values are less than or equal to y), where the median is equivalent to the 50% quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_count = df_groupby.count()\n",
    "df_count = df_count.rename(columns={'elapsedTime': 'count'})\n",
    "df_norm1 = df_groupby.median()\n",
    "df_norm1 = df_norm1.rename(columns={'elapsedTime': 'elapsedTimeMedian'})\n",
    "df_norm2 = df_groupby.mean()\n",
    "df_norm2 = df_norm2.rename(columns={'elapsedTime': 'elapsedTimeMean'})\n",
    "df_quant = df_groupby.quantile([0.8, 0.9]).unstack()\n",
    "df_quant.columns = ['%s%d%%' % (col[0], col[1] * 100) for col in df_quant.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.join(df_norm1).join(df_norm2).join(df_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add one more level to this and group the statistics by region, and then take a look at what the statistics look like on the different ticket statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groupby = df[['region', 'status', 'elapsedTime']].groupby(['region', 'status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll look at central tendency and quantiles. This time, though, these numbers are divided across the different Liferay regions, which means the table will be much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_count = df_groupby.count()\n",
    "df_count = df_count.rename(columns={'elapsedTime': 'count'})\n",
    "df_norm1 = df_groupby.median()\n",
    "df_norm1 = df_norm1.rename(columns={'elapsedTime': 'elapsedTimeMedian'})\n",
    "df_norm2 = df_groupby.mean()\n",
    "df_norm2 = df_norm2.rename(columns={'elapsedTime': 'elapsedTimeMean'})\n",
    "df_quant = df_groupby.quantile([0.8, 0.9]).unstack()\n",
    "df_quant.columns = ['%s%d%%' % (col[0], col[1] * 100) for col in df_quant.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.join(df_norm1).join(df_norm2).join(df_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plot Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's come back to our original question.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "One question you might ask yourself is, \"Should we copy another regions approach to handling tickets?\"\n",
    "\n",
    "Region-based groupings are useful when you are below the average (your ticket status times are higher than average), because it gives you opportunities to explore how your region differs from other regions and provides opportunities for that kind of process improvement.\n",
    "\n",
    "However, how do you know that your region is different from other regions? You could use a table, but a table is good for summarizing information, and not as good for pointing out differences.\n",
    "\n",
    "Another way to understand summary statistics is to visualize how the data is distributed. When looking at one-dimensional values (for example, the distribution of the time elapsed), this is often explained as a probability density function, which allows you to see how the data is concentrated. Peaks indicate that many values exist near that value.\n",
    "\n",
    "* [An introduction to kernel density estimation](http://www.mvstat.net/tduong/research/seminars/seminar-2001-05/)\n",
    "* [The idea of a probability density function](http://mathinsight.org/probability_density_function_idea)\n",
    "\n",
    "Neither histograms nor probability density functions are easy for non-mathematical people to interpret, so what you can do instead is use the probability density function to derive the cumulative distribution function. The values tell you how much of the data is less than a certain value, while the shape tells you how quickly that probability mass builds up.\n",
    "\n",
    "* [Cumulative distribution function](http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm)\n",
    "\n",
    "To plot these particular functions, we will be using the [seaborn](https://seaborn.pydata.org/) statistical data visualization library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Cumulative Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to read the cumulative distribution function, depending on the question you want to ask.\n",
    "\n",
    "The first question is, \"What percentage of tickets stay in the In Review status for 5 days or less?\" To answer this question, you search the \"Days in Status\" for 5, and then see where the vertical line intersects the In Review line.\n",
    "\n",
    "The second question is, \"Our goal is for 90% of tickets to spend 10 days or less In Review. What is the current cut-off point for 90% of tickets?\" To answer this question, you search the \"Proportion of Tickets\" for 90%, and then see where the horizontal line intersects the In Review line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df.groupby(['status']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], cumulative=True, bw=0.5, label=key)\n",
    "\n",
    "ax.set_xlim((0, 40))\n",
    "\n",
    "ax.set_yticks(np.linspace(0, 1, 11))\n",
    "ax.set_yticklabels(['%d%%' % x for x in np.linspace(0, 100, 11)])\n",
    "\n",
    "ax.set_xlabel('Days in Status')\n",
    "ax.set_ylabel('Proportion of Tickets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Cumulative Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say you wanted to answer the same question, but for a specific region, because we theoretically have more control over the results of our own region than we do over the entire department across multiple regions.\n",
    "\n",
    "To reiterate, the first question is, \"What percentage of tickets stay in the In Review status for 5 days or less *within my region of interest*?\" To answer this question, you search the \"Days in Status\" for 5, and then see where the vertical line intersects the In Review line.\n",
    "\n",
    "The second question is, \"Our goal is for 90% of tickets to spend 10 days or less In Review *within my region of interest*. What is the current cut-off point for 90% of tickets *within my region of interest*?\" To answer this question, you search the \"Proportion of Tickets\" for 90%, and then see where the horizontal line intersects the In Review line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df[df['region'] == 'US'].groupby(['status']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], cumulative=True, bw=0.5, label=key)\n",
    "\n",
    "ax.set_xlim((0, 40))\n",
    "\n",
    "ax.set_yticks(np.linspace(0, 1, 11))\n",
    "ax.set_yticklabels(['%d%%' % x for x in np.linspace(0, 100, 11)])\n",
    "\n",
    "ax.set_xlabel('Days in Status')\n",
    "ax.set_ylabel('Proportion of Tickets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a sense of how our ticket times are distributed. Let's come back to our research question.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "While we have some great summarization of our information, we haven't actually done any grouping of our data. Let's start with one of the most common groupings: time.\n",
    "\n",
    "When you group data over time and then visualize it, essentially you are monitoring is what is happening to a specific value, with data spaced out at equal intervals (hours, days, weeks, months, years), which opens up a lot of different ways to both analyze and transform the time data. The types of analyses you can do depend on the data set as well as the strength of your background in applied mathematics.\n",
    "\n",
    "* [Time series](https://en.wikipedia.org/wiki/Time_series)\n",
    "\n",
    "For the purposes of this notebook, we aren't going to try to analyze the data. Rather, we're going to try to apply a simple time-based summarization and visualize those summaries. Given the nature of our tickets, one easy time-based summarization is to look at the tickets created each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_bucket_plot(times, day_count, aggregator, label, solid_line=True):\n",
    "    if len(times) < 2:\n",
    "        return\n",
    "\n",
    "    sorted_times = sorted(\n",
    "        times,\n",
    "        lambda x, y: int(get_time_delta_as_days(x['issueCreated'] - y['issueCreated']))\n",
    "    )\n",
    "\n",
    "    if len(sorted_times) < 2:\n",
    "        return\n",
    "\n",
    "    start = 0\n",
    "    end = start\n",
    "\n",
    "    start_date = sorted_times[0]['issueCreated']\n",
    "    max_end_date = start_date + timedelta(days=day_count)\n",
    "\n",
    "    dates = []\n",
    "    values = []\n",
    "\n",
    "    while end + 1 < len(sorted_times):\n",
    "        while end + 1 < len(sorted_times) and sorted_times[end+1]['issueCreated'] < max_end_date:\n",
    "            end += 1\n",
    "\n",
    "        bucket = [entry['elapsedTime'] for entry in sorted_times[start:end+1]]\n",
    "\n",
    "        dates.append(start_date)\n",
    "        values.append(aggregator(bucket))\n",
    "\n",
    "        start = end + 1\n",
    "        end = start\n",
    "\n",
    "        start_date = max_end_date\n",
    "        max_end_date = start_date + timedelta(days=day_count)\n",
    "\n",
    "    if solid_line:\n",
    "        plt.plot_date(dates, values, fmt='-', label=label)\n",
    "    else:\n",
    "        plt.plot_date(dates, values, fmt='--', label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with this visualization, we'll focus on just those tickets that are in review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_review_aggregate_times = [entry for entry in aggregate_times if entry['status'] == 'In Review']\n",
    "unique_regions = set([entry['region'] for entry in aggregate_times])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also focus our graphs to a handful of regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_regions = ['US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for region in graph_regions:\n",
    "    assert region in unique_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Visualization: Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median will give us a sense of central tendency that is not as heavily influenced by extreme values as the mean. By plotting both the global information and the regional information on the same time plot, we get a sense of whether the central tendency in our region is similar to the central tendency of other regions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_bucket_plot(in_review_aggregate_times, 7, np.median, 'Global Median')\n",
    "\n",
    "for region in graph_regions:\n",
    "    region_times = [time for time in in_review_aggregate_times if time['region'] == region]\n",
    "    get_time_bucket_plot(region_times, 7, np.median, '%s Regional Median' % region, False)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Visualization: 90th Percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 90% line will tell us how close we are to meeting our goal. By plotting both the global information and the regional information on the same time plot, we get a sense of whether there are any noticeable differences and thus explain if our own region may have more trouble meeting the goal than the global tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nineties = functools.partial(np.percentile, q=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_bucket_plot(in_review_aggregate_times, 7, nineties, 'Global 90%')\n",
    "\n",
    "for region in graph_regions:\n",
    "    region_times = [time for time in in_review_aggregate_times if time['region'] == region]\n",
    "    get_time_bucket_plot(region_times, 7, np.median, '%s Regional 90%%' % region, False)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with time, you're not required to think of each data point only contributing to a single data point. With that in mind, let's remind ourselves of our original research question.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "What we've done is we've made it so that each data point only exists in one bucket. However, this isn't necessarily required. You can think of the data points having an impact on multiple buckets or groupings (which we represent as a time-based data point).\n",
    "\n",
    "To do that, one option is to have each data point affect all of the time-based data points that happen after it. When this impact is equal (so no matter how much time has elapsed, you still give it the same weight), you get the traditional statistics that we tabulated previously, just this value incorporates more and more data over time. This is known as a cumulative moving average.\n",
    "\n",
    "* [Cumulative moving average](https://en.wikipedia.org/wiki/Moving_average#Cumulative_moving_average)\n",
    "\n",
    "Another option is that this impact diminishes over time. For example, things that happen further and further in the past have less and less of an impact. A popular strategy to use is exponential decay, where you have a weighted moving average where dates further in the past have less and less of an impact. This is similar to Liferay's social activity implementation.\n",
    "\n",
    "* [The decaying average, relative and sensitive](http://donlehmanjr.com/Science/03%20Decay%20Ave/032.htm)\n",
    "\n",
    "For our visualization example, we're going to look at something that sits in between the two.\n",
    "\n",
    "The idea is that you have accumulate a traditional stastistic where you don't think too much about weight (so you don't define a time-based decay), but you don't want to use all data and instead decide to discard data after a certain amount of time has elapsed (giving it a weight of zero).\n",
    "\n",
    "The net effect is a simple moving average, which is implemented using a sliding (or rolling) time window.\n",
    "\n",
    "* [Simple moving average](https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/sma)\n",
    "\n",
    "If we choose 30 days for this sliding window, then for any given point in time, each value takes a look at all tickets opened thirty days before that point and computes a statistic describing how long those tickets took to close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_rolling_plot(times, window_size, aggregator, label, solid_line=True):\n",
    "    sorted_times = sorted(\n",
    "        times,\n",
    "        lambda x, y: int(get_time_delta_as_days(x['issueCreated'] - y['issueCreated']))\n",
    "    )\n",
    "\n",
    "    if len(sorted_times) < 2:\n",
    "        return\n",
    "\n",
    "    start = 0\n",
    "    end = 1\n",
    "\n",
    "    max_end_date = sorted_times[end]['issueCreated']\n",
    "    min_start_date = max_end_date - timedelta(days=window_size)\n",
    "\n",
    "    rolling_window = [sorted_times[end]['elapsedTime']]\n",
    "\n",
    "    dates = []\n",
    "    values = []\n",
    "\n",
    "    while end + 1 < len(sorted_times):\n",
    "        while start + 1 < end and sorted_times[start+1]['issueCreated'] < min_start_date:\n",
    "            rolling_window.pop(0)\n",
    "            start += 1\n",
    "\n",
    "        while end < len(sorted_times) and sorted_times[end]['issueCreated'] < max_end_date:\n",
    "            rolling_window.append(sorted_times[end]['elapsedTime'])\n",
    "            end += 1\n",
    "\n",
    "        dates.append(min_start_date)\n",
    "        values.append(aggregator(rolling_window))\n",
    "\n",
    "        min_start_date += timedelta(days=1)\n",
    "        max_end_date = min(today + timedelta(days=1), max_end_date + timedelta(days=1))\n",
    "\n",
    "    if solid_line:\n",
    "        plt.plot_date(dates, values, '-', label=label)\n",
    "    else:\n",
    "        plt.plot_date(dates, values, '--', label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window: Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a sliding window has a smoothing effect, and so while outliers will still have an effect (and they have an effect for a longer period of time), it is still customary to think of the mean first when using a sliding window.\n",
    "\n",
    "* [Smoothing algorithms](https://en.wikipedia.org/wiki/Smoothing#Smoothing_algorithms)\n",
    "\n",
    "Just as with our earlier plot, by plotting both the global information and the regional information on the same time plot, we get a sense of whether the central tendency in our region is similar to the central tendency of other regions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_rolling_plot(in_review_aggregate_times, 30, np.mean, 'Global Mean')\n",
    "\n",
    "for region in graph_regions:\n",
    "    region_times = [time for time in in_review_aggregate_times if time['region'] == region]\n",
    "    get_time_rolling_plot(region_times, 30, np.mean, '%s Regional Mean' % region, False)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window: 90th Percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the 90th percentile on a sliding window, you end up answering the question, \"How well have we been doing over the past month, and how has that varied over time?\" Having both global and regional values gives us a sense of how that compares to other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_rolling_plot(in_review_aggregate_times, 30, nineties, 'Global 90%')\n",
    "\n",
    "for region in graph_regions:\n",
    "    region_times = [time for time in in_review_aggregate_times if time['region'] == region]\n",
    "    get_time_rolling_plot(region_times, 30, nineties, '%s Regional 90%%' % region, False)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Group LPP Tickets by Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We've now finished up with a lot of introductory level visualizations. Let's remind ourselves of our original question.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "If we were to take a step back, the question we want to ask is: why do we care about groupings? Well, naturally because we believe that after we apply some grouping, it gives us something *actionable* that will impact our ticket times.\n",
    "\n",
    "One actioanble item might be, \"Do we need to restructure our teams to reflect the types of tickets we are receiving?\" In this case, you might consider component-based groupings to see if there are any components where you are performing below average (have higher status elapsed times), and thus deliver fixes more quickly by restructuring your teams to bring more resources into a given component, or improve the relationship between our support SMEs and engineering teams for those components to review those tickets more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Component-Based Grouping, Take 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine our component-based groupings, we have a few options. One of the easier options to simply generate the mapping table from the JIRA data we already have. All `components` tied to a JIRA issue are included in the `components` field. This gives us the following code to generate a mapping table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_component_names(issue):\n",
    "    for component in issue['fields']['components']:\n",
    "        yield component['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "component_mapping = [\n",
    "    {'jiraKey': key, 'component': component_name}\n",
    "        for key, issue in lpp_issues.items()\n",
    "            for component_name in get_component_names(issue)\n",
    "]\n",
    "\n",
    "df_component_mapping = pd.DataFrame(component_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have that component mapping table, we'll want to join it with our existing view that documents all of our elapsed times for every status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mapping_join = pd.merge(\n",
    "    df[df['status'] == 'In Review'],\n",
    "    df_component_mapping, on='jiraKey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Tabulation, Take 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we know for sure is that a lot of components are very rarely used. This means that data visualization by component might not actually be useful due to data sparsity. Let's confirm that by creating a cross-tabulation of status by component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    df_mapping_join['component'],\n",
    "    df_mapping_join['status']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cross-tabulation aligns with the initial belief that what we have is extremely sparse. That means, we'll want to increase the coarsity of the component if we want to understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Component-Based Grouping, Take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of ways to redo the component-based grouping, all of them rooted in different ways of organizing and retrieving information, and all of them valid depending on the context.\n",
    "\n",
    "* [Information science](https://en.wikipedia.org/wiki/Information_science)\n",
    "\n",
    "In our case, where different categorizations have been created based on Engineering teams, the most obvious is to simply use the Engineering team names that Liferay has defined, where everything following the greater than the sign is stripped. We'll want to make sure that we don't return the same value twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parent_component_names(issue):\n",
    "    component_names = [\n",
    "        component['name'] for component in issue['fields']['components']\n",
    "    ]\n",
    "\n",
    "    parent_component_names = set([\n",
    "        component_name[0:component_name.find(' > ')] if component_name.find(' > ') > 0 else component_name\n",
    "            for component_name in component_names\n",
    "    ])\n",
    "\n",
    "    return parent_component_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parent_component_mapping = [\n",
    "    {'jiraKey': key, 'component': component_name}\n",
    "        for key, issue in lpp_issues.items()\n",
    "            for component_name in get_parent_component_names(issue)\n",
    "]\n",
    "\n",
    "df_parent_component_mapping = pd.DataFrame(parent_component_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to join our new mapping table with our existing view that documents all of our elapsed times for every status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_parent_component_mapping_join = pd.merge(\n",
    "    df[df['status'] == 'In Review'],\n",
    "    df_parent_component_mapping,\n",
    "    on='jiraKey'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Tabulation, Take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we addressed our data sparsity by creating a cross-tabulation of status by component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parent_component_crosstab = pd.crosstab(\n",
    "    df_parent_component_mapping_join['component'],\n",
    "    df_parent_component_mapping_join['status']\n",
    ")\n",
    "\n",
    "df_parent_component_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we still have a sparsity problem if we decide to look only at the \"In Review\" column. Therefore, we'll want to consolidate our groupings even more if we want to really understand what is happening. Should we do so, we need to keep in mind that we also run the risk of over-summarizing and losing too much contextual information.\n",
    "\n",
    "For now, what we'll do is we'll only explore the top 5 components, because that's the number of colors we have available in our color-blind friendly palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parent_component_crosstab[\n",
    "    df_parent_component_crosstab['In Review'] > 20\n",
    "].sort_values('In Review').tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, of course, overlap exactly with what we know to be components where bug reports frequently arive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highcount_components = df_parent_component_crosstab[\n",
    "    df_parent_component_crosstab['In Review'] > 20\n",
    "].sort_values('In Review').tail(5).index.values\n",
    "\n",
    "highcount_join = df_parent_component_mapping_join[\n",
    "    df_parent_component_mapping_join['component'].isin(highcount_components)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, note that this isn't the only way to derive components.\n",
    "\n",
    "An alternate approach is to derive brand new groupings the same way a search engine might derive groupings. After all, each ticket has descriptions and comments, so you might try to derive new categorizations based on summarizations of those descriptions and comments (such as tags) rather than rely on the existing Liferay categorizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize LPP Tickets by Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to bring everything together to answer our research question.\n",
    "\n",
    "<b style=\"color:green\">Can we create data visualizations on top of LPS ticket metadata that let us group together different DXP tickets so that we can explore the times that tickets remain \"In Review\" based on those groupings?</b>\n",
    "\n",
    "Now that we have derived the grouping (component) by performing a join, we will apply the same visualizations that we used for regions to instead compare the \"In Review\" times for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby = highcount_join[['component', 'elapsedTime']].groupby(['component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_count = df_groupby.count()\n",
    "df_count = df_count.rename(columns={'elapsedTime': 'count'})\n",
    "df_norm1 = df_groupby.median()\n",
    "df_norm1 = df_norm1.rename(columns={'elapsedTime': 'elapsedTimeMedian'})\n",
    "df_norm2 = df_groupby.mean()\n",
    "df_norm2 = df_norm2.rename(columns={'elapsedTime': 'elapsedTimeMean'})\n",
    "df_quant = df_groupby.quantile([0.8, 0.9]).unstack()\n",
    "df_quant.columns = ['%s%d%%' % (col[0], col[1] * 100) for col in df_quant.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.join(df_norm1).join(df_norm2).join(df_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Cumulative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in highcount_join.groupby(['component']):\n",
    "    ax = sns.kdeplot(group['elapsedTime'], cumulative=True, bw=0.5, label=key)\n",
    "\n",
    "ax.set_xlim((0, 40))\n",
    "\n",
    "ax.set_yticks(np.linspace(0, 1, 11))\n",
    "ax.set_yticklabels(['%d%%' % x for x in np.linspace(0, 100, 11)])\n",
    "\n",
    "ax.set_xlabel('Days In Review')\n",
    "ax.set_ylabel('Proportion of Tickets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Visualization: Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component_name in highcount_components:\n",
    "    component_tickets = highcount_join[highcount_join['component'] == component_name]['jiraKey'].values\n",
    "    component_times = [time for time in in_review_aggregate_times if time['jiraKey'] in component_tickets]\n",
    "\n",
    "    get_time_bucket_plot(component_times, 30, np.median, '%s Median' % component_name)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Visualization: 90th Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component_name in highcount_components:\n",
    "    component_tickets = highcount_join[highcount_join['component'] == component_name]['jiraKey'].values\n",
    "    component_times = [time for time in in_review_aggregate_times if time['jiraKey'] in component_tickets]\n",
    "\n",
    "    get_time_bucket_plot(component_times, 30, nineties, '%s 90%%' % component_name)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window: Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component_name in highcount_components:\n",
    "    component_tickets = highcount_join[highcount_join['component'] == component_name]['jiraKey'].values\n",
    "    component_times = [time for time in in_review_aggregate_times if time['jiraKey'] in component_tickets]\n",
    "\n",
    "    get_time_rolling_plot(component_times, 90, np.mean, '%s Mean' % component_name)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window: 90th Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component_name in highcount_components:\n",
    "    component_tickets = highcount_join[highcount_join['component'] == component_name]['jiraKey'].values\n",
    "    component_times = [time for time in in_review_aggregate_times if time['jiraKey'] in component_tickets]\n",
    "\n",
    "    get_time_rolling_plot(component_times, 90, nineties, '%s 90%%' % component_name)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
