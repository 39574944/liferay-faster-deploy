{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 1: Combining JIRA With GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, our research question is as follows:\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "In order to answer this question, this notebook introduces database joins by creating a script that combines data retrieved from JIRA with data retrieved from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y numpy pandas pytz requests ujson\n",
    "!pip install dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "import dateparser\n",
    "from datetime import date, datetime\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import re\n",
    "import requests\n",
    "import six\n",
    "import sys\n",
    "import subprocess\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we'll make sure that we establish one rule for this script and all future scripts: we will save all raw data.\n",
    "\n",
    "This is important, because one of the more common things to do as a developer is to retrieve the data, extract only the information you need, and then discard the data you do not need. However, unless you have some terms of service agreement restricting what data you are allowed to retain, do not discard the raw data! Raw data is a starting point that speeds up the creation of many different application prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "now = datetime.now(pytz.utc)\n",
    "\n",
    "def load_raw_data(cache_name):\n",
    "    base_name = os.path.basename(cache_name)\n",
    "    subfolder_name = os.path.dirname(cache_name)\n",
    "    folder_name = 'rawdata/%s' % subfolder_name\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        return None\n",
    "\n",
    "    file_name = '%s/%s_%s.json' % (folder_name, today.isoformat(), base_name)\n",
    "\n",
    "    if not os.path.isfile(file_name):\n",
    "        return None\n",
    "\n",
    "    with open(file_name) as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "def save_raw_data(cache_name, json_value):\n",
    "    base_name = os.path.basename(cache_name)\n",
    "    subfolder_name = os.path.dirname(cache_name)\n",
    "    folder_name = 'rawdata/%s' % subfolder_name\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    file_name = '%s/%s_%s.json' % (folder_name, today.isoformat(), base_name)\n",
    "\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(json_value, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from JIRA, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep our question in mind during each step.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Before we can answer the question of whether anything is *stuck in review*, we will need to answer the broader question of what is *in review*. We'll start by looking at what's in review in JIRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to JIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because [issues.liferay.com](https://issues.liferay.com/) does not have OAuth support, we will need to find a different way to connect to our JIRA installation. The simplest way is to simply login to JIRA.\n",
    "\n",
    "There are a lot of secure ways to specify your username and password, but for the sake of this script, we'll use the most insecure way possible: a plain text file. Namely, the `.gitconfig` in your user's home folder. If you want to use a different strategy that's less global (a plain text JSON file in the same folder as this script, for example) or more secure, just change the implementation of the two functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_config(key):\n",
    "    try:\n",
    "        return subprocess.check_output(['git', 'config', key]).strip().decode('utf8')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def set_config(key, value):\n",
    "    subprocess.call(['git', 'config', '--global', key, value])\n",
    "    subprocess.call(['git', 'config', '--global', key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you are using the default implementation, set your username and password inside of the `.gitconfig` located in your user's home folder by running the following two commands in a command line window.\n",
    "\n",
    "``` .sh\n",
    "git config --global jira.session-username JIRA_USERNAME\n",
    "git config --global jira.session-password JIRA_PASSWORD\n",
    "```\n",
    "\n",
    "The following cell will read it in and confirm that it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jira_username = get_config('jira.session-username')\n",
    "jira_password = get_config('jira.session-password')\n",
    "\n",
    "assert(jira_username is not None)\n",
    "assert(jira_password is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will use this information and attempt to login to JIRA and confirm that it has a valid session, which will confirm that the credentials you saved are valid. It will also save this session information so that it can reuse it later without constantly relogging in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jira_base_url = 'https://issues.liferay.com/rest'\n",
    "\n",
    "def get_jira_cookie():\n",
    "    jira_cookie = None\n",
    "\n",
    "    jira_cookie_name = None\n",
    "    jira_cookie_value = None\n",
    "\n",
    "    try:\n",
    "        jira_cookie_name = get_config('jira.session-cookie-name')\n",
    "        jira_cookie_value = get_config('jira.session-cookie-value')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if jira_cookie_name is not None and jira_cookie_value is not None:\n",
    "        jira_cookie = {\n",
    "            jira_cookie_name: jira_cookie_value\n",
    "        }\n",
    "\n",
    "        r = requests.get(jira_base_url + '/auth/1/session', cookies=jira_cookie)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            jira_cookie = None\n",
    "\n",
    "    if jira_cookie is not None:\n",
    "        return jira_cookie\n",
    "        \n",
    "    post_json = {\n",
    "        'username': jira_username,\n",
    "        'password': jira_password\n",
    "    }\n",
    "\n",
    "    r = requests.post(jira_base_url + '/auth/1/session', json=post_json)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print('Invalid login')\n",
    "\n",
    "        return None\n",
    "\n",
    "    response_json = r.json()\n",
    "\n",
    "    jira_cookie_name = response_json['session']['name']\n",
    "    jira_cookie_value = response_json['session']['value']\n",
    "\n",
    "    set_config('jira.session-cookie-name', jira_cookie_name)\n",
    "    set_config('jira.session-cookie-value', jira_cookie_value)\n",
    "\n",
    "    jira_cookie = {\n",
    "        jira_cookie_name: jira_cookie_value\n",
    "    }\n",
    "\n",
    "    return jira_cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(get_jira_cookie() is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve JIRA Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a valid login, our next step is to use the JIRA API to retrieve tickets. If you've interacted with JIRA before, you know that it has its own query language (JQL). It turns out there is a simple search API that allows you to submit the JQL and all the matching issues are returned as JSON. Since the API is fairly simple, we implement it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_jira_issues(jql):\n",
    "    jira_cookie = get_jira_cookie()\n",
    "\n",
    "    if jira_cookie is None:\n",
    "        return []\n",
    "\n",
    "    start_at = 0\n",
    "\n",
    "    post_json = {\n",
    "        'jql': jql,\n",
    "        'startAt': start_at\n",
    "    }\n",
    "\n",
    "    r = requests.post(jira_base_url + '/api/2/search', cookies=jira_cookie, json=post_json)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    response_json = r.json()\n",
    "\n",
    "    issues = response_json['issues']\n",
    "\n",
    "    while start_at + response_json['maxResults'] < response_json['total']:\n",
    "        start_at += response_json['maxResults']\n",
    "        post_json['startAt'] = start_at\n",
    "\n",
    "        r = requests.post(jira_base_url + '/api/2/search', cookies=jira_cookie, json=post_json)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return issues\n",
    "\n",
    "        response_json = r.json()\n",
    "\n",
    "        issues.extend(response_json['issues'])\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have something that can retrieve JIRA issues, all we need is to actually create our JQL and then run the search. This is the JQL we'll use for regular Liferay Portal Patch (LPP) issues that are in review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_review_jql = '''\n",
    "    project = LPP AND\n",
    "    type not in (\"SME Request\", \"SME Request SubTask\") AND\n",
    "    status = \"In Review\"\n",
    "    order by key\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and retrieve those results. Looking at JSON is a bit tedious, so we'll take a look at a subset of fields in a way that resembles the view you get when you run JQL via the web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jql_hashes = load_raw_data('jql_hashes')\n",
    "\n",
    "if jql_hashes is None:\n",
    "    jql_hashes = {}\n",
    "\n",
    "def get_jql_hashed_name(base_name, jql):\n",
    "    jql_hash = None\n",
    "    \n",
    "    for key, value in jql_hashes.items():\n",
    "        if value == jql:\n",
    "            jql_hash = key\n",
    "            break\n",
    "\n",
    "    if jql_hash is None:            \n",
    "        digester = hashlib.md5()\n",
    "        digester.update(jql)\n",
    "        jql_hash = digester.hexdigest()\n",
    "\n",
    "        jql_hashes[jql_hash] = jql\n",
    "\n",
    "        save_raw_data('jql_hashes', jql_hashes)\n",
    "    \n",
    "    return '%s/%s' % (jql_hash, base_name)\n",
    "\n",
    "def get_jira_issues(jql):\n",
    "    base_name = get_jql_hashed_name('jira_issues', jql)\n",
    "\n",
    "    jira_issues = load_raw_data(base_name)\n",
    "\n",
    "    if jira_issues is not None:\n",
    "        print('Loaded cached JIRA search')\n",
    "        return jira_issues\n",
    "\n",
    "    print('Executing JIRA search')\n",
    "\n",
    "    jira_issues = retrieve_jira_issues(in_review_jql)\n",
    "    save_raw_data(base_name, jira_issues)\n",
    "    return jira_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    jira_issues = get_jira_issues(in_review_jql)\n",
    "else:\n",
    "    jira_issues = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JIRAIssue = namedtuple(\n",
    "    'JIRAIssue',\n",
    "    ['key', 'region', 'status', 'assignee', 'summary']\n",
    ")\n",
    "\n",
    "def get_jira_tuple(issue):\n",
    "    region_field_name = 'customfield_11523'\n",
    "\n",
    "    regions = ['']\n",
    "\n",
    "    if region_field_name in issue['fields']:\n",
    "        regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "\n",
    "    return JIRAIssue(\n",
    "        key=issue['key'],\n",
    "        region=regions[0],\n",
    "        status=issue['fields']['status']['name'],\n",
    "        assignee=issue['fields']['assignee']['displayName'],\n",
    "        summary=issue['fields']['summary']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([get_jira_tuple(item) for item in jira_issues])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Derived Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the result of executing the previous code cell, the result of our API call against JIRA using JQL can be conceptualized as a table.\n",
    "\n",
    "Just as lists of results can be conceptualized as a table, so too are things that work with regular SQL. More explicitly, whenever you're working with a database and you execute SQL, the result can be understood to be another table, whether this table consists of a single value, a single row, or multiple rows.\n",
    "\n",
    "* [Relational algebra](https://en.wikipedia.org/wiki/Relational_algebra)\n",
    "\n",
    "At a conceptual level, running database queries is taking existing tables and generating (or deriving) new tables that better fit the question you are trying to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting Derived Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last point is worth emphasizing, as it comes up in upgrade performance tuning. In short, running database queries always generates new tables. More explicitly, the database will have gone through all of the effort required to create a table when executing your query, skipping only the persistence step.\n",
    "\n",
    "* [The difference between subqueries and derived tables in SQL](https://www.xaprb.com/blog/2005/09/26/sql-subqueries-and-derived-tables/)\n",
    "\n",
    "Looking at this a little differently, imagine that we decided to remove the code that invoked the `load_raw_data` and `save_raw_data` functions. Would this have noticeably improved the performance of our code? Not really, because the retrieval from JIRA is slower than writing the data to disk. That's equivalent to the cost difference between persisting the data and not persisting it.\n",
    "\n",
    "Having the derived table be saved as an permanent table also allows you to add metadata, such as indices, that will improve the performance on repeated queries. So, in cases where you are operating on different subsets of a larger subset of the data, and this larger subset is expensive to compute, it's actually very sensible to save your derived table to reduce query execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from GitHub, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of our original question.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "What is this concept of \"in review\"? Well, being \"in review\" actually means that your code has been written and you are now waiting on another team member to look at your changes, providing a sanity check from someone who is looking at the solution with fresh eyes.\n",
    "\n",
    "Where do these sanity check reviews occur? They occur on GitHub. This means that we will want to bring in the GitHub data set in order to answer our question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, [api.github.com](https://developer.github.com/v3/) does have OAuth support, so we'll want to request an OAuth token from GitHub and leverage it in our script.\n",
    "\n",
    "* [Creating a personal access token for command line](https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/)\n",
    "\n",
    "Assuming you are using the default implementation for configuration values provided by this notebook (mentioned earlier when setting up JIRA access), set your OAuth token inside of the `.gitconfig` located in your user's home folder by running the following command in a command line window.\n",
    "\n",
    "``` .sh\n",
    "git config --global github.oauth-token GITHUB_OAUTH_TOKEN\n",
    "```\n",
    "\n",
    "If you customized it, do whatever you need to get the configuration value persisted. The following cell will read it in and confirm that it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "github_oauth_token = get_config('github.oauth-token')\n",
    "\n",
    "assert(github_oauth_token is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's not enough that it exists. We should confirm that the token works on the repositories that are related to Liferay. We'll use the `liferay/liferay-portal` and `liferay/liferay-portal-ee` repositories as a way to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_base_url = 'https://api.github.com'\n",
    "\n",
    "def is_repository_accessible(reviewer_url):\n",
    "    print('Validating OAuth token against %s' % reviewer_url)\n",
    "\n",
    "    headers = {\n",
    "        'user-agent': 'python checklpp.py',\n",
    "        'authorization': 'token %s' % github_oauth_token\n",
    "    }\n",
    "\n",
    "    api_path = '/repos/%s' % reviewer_url\n",
    "    \n",
    "    r = requests.get(github_base_url + api_path, headers=headers)\n",
    "    \n",
    "    return r.status_code == 200\n",
    "\n",
    "assert(is_repository_accessible('liferay/liferay-portal'))\n",
    "assert(is_repository_accessible('liferay/liferay-portal-ee'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Join Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have one data set (a list of JIRA tickets that are in review), and our next step is to think about how we can answer our actual question.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Now that we've identified which LPP tickets are currently in review, our next step is to identify pull requests that are stuck in review. So, that leaves us with the following question: how should we implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of Query Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will happen from here is you ask the database to provide an explanation of what it's doing for your query, you work some magic to make this explanation look better (add database indices, modify the query), and you commit those changes to the codebase.\n",
    "\n",
    "* [Query plan](https://en.wikipedia.org/wiki/Query_plan)\n",
    "\n",
    "Our definition of \"better\" comes from interpreting certain aspects of the query plan as necessarily worse than what we expected. Our expectations often come from definitions that are provided by various database vendors on the ideal query plan.\n",
    "\n",
    "* [MySQL explain plan](https://dev.mysql.com/doc/refman/5.6/en/explain-output.html)\n",
    "* [Oracle explain plan](https://docs.oracle.com/database/121/TGSQL/tgsql_interp.htm)\n",
    "* [PostgreSQL explain plan](http://www.postgresql.org/docs/9.4/static/using-explain.html)\n",
    "* [SQL Server explain plan](https://technet.microsoft.com/en-us/library/ms178071%28v=sql.105%29.aspx)\n",
    "\n",
    "When you run into a slow query and you ask for it to provide an explanation of its approach, the output you receive is the query plan derived by the query optimizer.\n",
    "\n",
    "* [Query optimization](https://en.wikipedia.org/wiki/Query_optimization)\n",
    "\n",
    "Query optimization plans essentially evaluate various ways of correctly answering the query described by the SQL statement while also minimizing the cost of loading the query. For a database, this notion of \"cost\" can be summarized as use of memory (in particular, pages getting swapped in and out of active memory), disk I/O, and network I/O.\n",
    "\n",
    "* [Relative time cost of computer operations](http://norvig.com/21-days.html#answers)\n",
    "\n",
    "The result of this optimized plan particularly easy to understand in Microsoft SQL Server, where it gives you a graphical representation of its query plan, and it estimates what percentage of the time spent processing the query will be spent on that specific aspect of it.\n",
    "\n",
    "* [SQL Server query execution plans](http://www.sqlshack.com/sql-server-query-execution-plans-understanding-reading-plans/)\n",
    "* [SQL Server graphical plan icons](https://technet.microsoft.com/en-us/library/ms175913%28v=sql.105%29.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of Join Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a step back, what are we doing is that we need to derive new information from two separate data sources. If these two data sources were both tables, then if these tables have columns that refer to the same abstract concept (or perhaps a third \"mapping table\" that describes that abstract concept), then we would join these two \n",
    "\n",
    "* [Khan Academy: Joining Related Tables](https://www.khanacademy.org/computing/computer-programming/sql/relational-queries-in-sql/p/joining-related-tables)\n",
    "\n",
    "We've identified all of the JIRA issues that are in review. Presumably, we want to then combine this with GitHub data. What algorithm will we implement to achieve this? It turns out there are three basic strategies for computing a join: a nested loop join, a hash join, and a sort-merge join.\n",
    "\n",
    "* [Join methods and subqueries](http://www.orafaq.com/tuningguide/join%20methods.html)\n",
    "\n",
    "Sort-merge deserves its own discussion (which we'll do in the next tutorial), and it will make more sense after we're finished than before we've finished. So we'll look first at how a nested loop join and a hash join relate to our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Loop Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple solution to this problem would be to iterate over each of our pull requests and then check each JIRA ticket to see if the JIRA ticket references the pull request. So for every GitHub pull request, you would check every JIRA ticket. This strategy is known as a **nested loop join**.\n",
    "\n",
    "A nested loop join is a join where the query optimizer decides the best way to accomplish the join is to designate one table as the \"outer table\" (no connection to the notion of an outer join) and the other table as the \"inner table\", structured in much the same way as a for loop.\n",
    "\n",
    "* [Nested loop join](https://en.wikipedia.org/wiki/Nested_loop_join)\n",
    "\n",
    "How fast is a nested loop join? Let $m$ be the size of the JIRA table and $n$ be the size of the GitHub table. Regardless of which table you choose for the outer table, a nested loop join would have an expected runtime and a worst-case runtime of $O(m \\cdot n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to this problem would be to first load all the pull requests into a lookup data structure. From there, iterate over the JIRA tickets and match the pull requests to the hash table. So for every JIRA ticket, you would perform a number of lookups against our data structure not based on either tables size (so, effectively a constant). This strategy is known as a **hash join**.\n",
    "\n",
    "A hash join is a join where the query optimizer decides the best way to accomplish the join is to build a lookup table on the smaller table (because it's more likely that a smaller table can fit into memory), with a popular choice being a hash table, and then iterate over the larger table.\n",
    "\n",
    "* [Hash join](https://en.wikipedia.org/wiki/Hash_join)\n",
    "\n",
    "How fast is a hash join? Let $m$ be the size of the JIRA table and $n$ be the size of the GitHub table. Note that hash tables have an $O(1)$ expected access and insertion time, but an $O(n)$ worst case access and insertion time (due to hash collisions, table resizes). So in practice, we have an expected runtime that is $O(m) + O(n)$ and a worst-case runtime that is $O(m \\cdot n) + O(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from GitHub, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a valid login, we can proceed with our question.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "As noted previously, before we can answer the question of whether anything is *stuck in review*, we will need to answer the broader question of what is *in review*. Now that we know what's in review in JIRA, we also want to what's in review on GitHub. Then, we will want to join these two tables together.\n",
    "\n",
    "Between a nested loop join and a hash join, the hash join has the better expected runtime. So how can we perform a hash join? We need to create a lookup data structure containing all the pull requests using some key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve GitHub Pull Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, given the number of unique users repositories that are involved in reviewing Liferay pull requests, it's not practical to make an API call against every user and every repository. Instead, we'll want to do this on-demand based on which pull requests we know exist.\n",
    "\n",
    "If you visit `/repos/USERNAME/REPOSITORY/pulls` without a pull ID, GitHub will return all currently open pull requests. The approach below uses this API in an attempt to reduce the number of API requests to GitHub, because this will fetch all pull requests that are open in a single request.\n",
    "\n",
    "Following our principle of keeping any data we retrieve, we save all of these pull requests, and then we request any additional pull requests that are closed and save those as well. Because our end goal is a hash join, we'll load this table as a map or dictionary where the key is the GitHub URL, because that's what will be present in JIRA fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_pull_requests(reviewer_url, pull_request_ids=[]):\n",
    "    print('Checking pull requests waiting on %s' % reviewer_url)\n",
    "\n",
    "    headers = {\n",
    "        'user-agent': 'python checklpp.py',\n",
    "        'authorization': 'token %s' % github_oauth_token\n",
    "    }\n",
    "\n",
    "    api_path = '/repos/%s/pulls' % reviewer_url\n",
    "\n",
    "    r = requests.get(github_base_url + api_path, headers=headers)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return {}\n",
    "\n",
    "    new_pull_requests = r.json()\n",
    "\n",
    "    new_seen_pull_requests = { pull_request['html_url']: pull_request for pull_request in new_pull_requests }\n",
    "\n",
    "    for pull_request_id in pull_request_ids:\n",
    "        github_url = 'https://github.com/%s/pull/%s' % (reviewer_url, pull_request_id)\n",
    "\n",
    "        if github_url in new_seen_pull_requests:\n",
    "            continue\n",
    "\n",
    "        api_path = '/repos/%s/pulls/%s' % (reviewer_url, pull_request_id)\n",
    "\n",
    "        r = requests.get(github_base_url + api_path, headers=headers)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        new_seen_pull_requests[github_url] = r.json()\n",
    "\n",
    "    return new_seen_pull_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll take a look at all the pull requests open against `liferay/liferay-portal-ee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_open_backports():\n",
    "    open_backports = load_raw_data('open_backports')\n",
    "\n",
    "    if open_backports is not None:\n",
    "        print('Loaded cached open backports')\n",
    "        return open_backports\n",
    "        \n",
    "    open_backports = retrieve_pull_requests('liferay/liferay-portal-ee')\n",
    "    save_raw_data('open_backport_pulls', open_backports)\n",
    "    return open_backports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    open_backports = get_open_backports()\n",
    "else:\n",
    "    open_backport_pulls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GHPullRequest = namedtuple(\n",
    "    'GHPullRequest',\n",
    "    ['submitter', 'pull_id', 'branch', 'created_at', 'updated_at', 'state', 'github_url']\n",
    ")\n",
    "\n",
    "def get_github_tuple(pull_request):\n",
    "    pull_id = '%s/%s#%d' % (\n",
    "        pull_request['base']['user']['login'],\n",
    "        pull_request['base']['repo']['name'],\n",
    "        pull_request['number']\n",
    "    )\n",
    "    \n",
    "    return GHPullRequest(\n",
    "        submitter=pull_request['user']['login'],\n",
    "        pull_id=pull_id,\n",
    "        branch=pull_request['base']['ref'],\n",
    "        created_at=pull_request['created_at'],\n",
    "        updated_at=pull_request['updated_at'],\n",
    "        state=pull_request['state'],\n",
    "        github_url=pull_request['url']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([get_github_tuple(pull_request) for pull_request in open_backports.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Database Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database indices are much easier to understand at an implementation level after you understand sort-merge joins, but we're going to delay that discussion until we have some more background. Instead, we'll focus on the concept behind a database index.\n",
    "\n",
    "* [Database index](https://en.wikipedia.org/wiki/Database_index)\n",
    "\n",
    "\n",
    "```\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from JIRA, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a lookup data structure for GitHub, we look back at our question.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Well, the thing to remember here is that the database query can go either way. Just as you might want to lookup GitHub pull requests by their URL, you will also want to lookup JIRA tickets by their ticket key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jira_issues_by_key = {issue['key']: issue for issue in jira_issues}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Mapping Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function that can retrieve GitHub pull request metadata, you could say that in addition to having a JIRA table and a GitHub table. However, how can we join the two together?\n",
    "\n",
    "The answer is a strategy that you see implemented in Liferay Service Builder. This strategy is known as a mapping table, named for how it explicitly creates many-to-many mappings.\n",
    "\n",
    "* [Understanding Mapping Tables](https://stackoverflow.com/questions/6453462/mysql-understanding-mapping-tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Table Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume you have two tables, JIRA and GitHub You can use a mapping table in order to document the primary keys of JIRA and how they relate to the primary keys on GitHub.\n",
    "\n",
    "What kinds of question can you answer with a mapping table? Simplistically, you could fetch all JIRA tickets corresponding to a specific GitHub pull request and all GitHub pull requests corresponding to a specific JIRA ticket. If you perform a three-way join, you could get all JIRA tickets and all GitHub pull requests that reference each other.\n",
    "\n",
    "To answer these questions, you have to think more carefully about how to optimize these queries. A simple solution that works on smaller data sets is to load the entire mapping table into memory and perform hash joins on both sides.\n",
    "\n",
    "However, it turns out that you can do better. There is a logical extension of a mapping table that brings together more than two keys, and this extension is referred to as a star schema. These are actually common in data warehouses, and databases can be configured (usually through the use of specialized database index types) to drastically improve query cost plans against star schemas.\n",
    "\n",
    "* [Star schema](https://en.wikipedia.org/wiki/Star_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from GitHub, Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have our lookup data structure, what do we need to answer our question?\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Well, while we have a lookup data structure, it turns out we don't actually have a GitHub table yet, only a subset of the GitHub table corresponding to pull requests sent against `liferay/liferay-portal-ee`! So our next step is to generate the relevant portion of our GitHub table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify GitHub Pulls Related to JIRA Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while we have a utility method that allows us to retrieve GitHub pull request histories, we don't actually have most of that information. So how do we identify which ones to retrieve?\n",
    "\n",
    "Our first step is to iterate over each JIRA ticket and extract the pull requests contained in the ticket. Once we have that information, we'll be able to use our lookup data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_jira_pull_request_urls(jira_issues):\n",
    "    issues_by_request = defaultdict(set)\n",
    "    requests_by_reviewer = defaultdict(set)\n",
    "\n",
    "    for jira_issue in jira_issues:\n",
    "        for value in jira_issue['fields'].values():\n",
    "            if not isinstance(value, six.string_types):\n",
    "                continue\n",
    "\n",
    "            for github_url in re.findall('https://github.com/[^\\s]*/pull/[\\d]+', value):\n",
    "                issues_by_request[github_url].add(jira_issue['key'])\n",
    "\n",
    "                pos = github_url.find('/', github_url.find('/', 19) + 1)\n",
    "                reviewer_url = github_url[19:pos]\n",
    "                pull_request_id = github_url[github_url.rfind('/') + 1:]\n",
    "\n",
    "                requests_by_reviewer[reviewer_url].add(pull_request_id)\n",
    "\n",
    "    return issues_by_request, requests_by_reviewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pass the data the results of our previous JIRA search to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jira_pull_request_urls(jql):\n",
    "    base_name_1 = get_jql_hashed_name('issues_by_request', jql)\n",
    "    base_name_2 = get_jql_hashed_name('requests_by_reviewer', jql)\n",
    "    \n",
    "    issues_by_request = load_raw_data(base_name_1)\n",
    "    requests_by_reviewer = load_raw_data(base_name_2)\n",
    "\n",
    "    if issues_by_request is not None and requests_by_reviewer is not None:\n",
    "        print('Loaded cached JIRA to GitHub mapping')\n",
    "        return issues_by_request, requests_by_reviewer\n",
    "    \n",
    "    jira_issues = get_jira_issues(jql)\n",
    "    issues_by_request, requests_by_reviewer = extract_jira_pull_request_urls(jira_issues)\n",
    "\n",
    "    save_raw_data(base_name_1, issues_by_request)\n",
    "    save_raw_data(base_name_2, requests_by_reviewer)\n",
    "    \n",
    "    return issues_by_request, requests_by_reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    issues_by_request, requests_by_reviewer = get_jira_pull_request_urls(in_review_jql)\n",
    "else:\n",
    "    issues_by_request = {}\n",
    "    requests_by_reviewer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JIRAGitHubMapping = namedtuple('JIRAGitHubMapping', ['jiraKey', 'githubKey'])\n",
    "\n",
    "pd.DataFrame([\n",
    "    JIRAGitHubMapping(jiraKey=jiraKey, githubKey=githubKey)\n",
    "        for githubKey, jiraKeys in issues_by_request.items()\n",
    "            for jiraKey in jiraKeys\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it turns out that what we've done is equivalent to building a mapping table between the JIRA tickets (represented with their issue key) and the GitHub pull requests (represented with their URL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve GitHub Pulls Related to JIRA Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll want to fetch all the metadata associated with all those pull requests. We'll also want a quick way to separate the open/active pull requests from the closed/inactive pull requests so that we don't have to re-derive that metadata later on when we attempt to identify stuck pull requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_related_pull_requests(issues_by_request, requests_by_reviewer):\n",
    "    seen_pull_requests = {}\n",
    "\n",
    "    for reviewer_url, pull_request_ids in sorted(requests_by_reviewer.items()):\n",
    "        new_seen_pull_requests = retrieve_pull_request_reviews(reviewer_url, pull_request_ids)\n",
    "        seen_pull_requests.update(new_seen_pull_requests)\n",
    "\n",
    "    return seen_pull_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and use our mapping tables to populate the subset of the GitHub table corresponding to only (1) currently active pull requests, or (2) closed pull requests corresponding to a Liferay Portal Patch ticket that is currently in review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_pull_requests(jql):\n",
    "    base_name = get_jql_hashed_name('seen_pull_requests', jql)\n",
    "    \n",
    "    seen_pull_requests = load_raw_data(base_name)\n",
    "\n",
    "    if seen_pull_requests is not None:\n",
    "        print('Loaded cached pull request metadata')\n",
    "        return seen_pull_requests\n",
    "\n",
    "    issues_by_request, requests_by_reviewer = extract_jira_pull_request_urls(jql)\n",
    "\n",
    "    seen_pull_requests = retrieve_related_pull_requests(issues_by_request, requests_by_reviewer)\n",
    "    save_raw_data(base_name, seen_pull_requests)\n",
    "    \n",
    "    return seen_pull_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    seen_pull_requests = get_related_pull_requests(in_review_jql)\n",
    "else:\n",
    "    seen_pull_requests = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([get_github_tuple(pull_request) for pull_request in seen_pull_requests.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform JIRA-GitHub Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have raw data from both JIRA and GitHub, excluding comments. How can we proceed in answering our original question?\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Our next question will be, what pull requests are still open for each LPP ticket? From there, we'll then be able to look for tickets that are stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open LPP Pull Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed by identifying the open pull requests that are still open (scanning the left table and filtering). We can then use our mapping table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Computed Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this, now we need to now answer the following question: given a list of LPP tickets and given a list of pull requests, how do we identify the ones that are *stuck*? Well, that's something we can derive by checking how long pull requests have stayed open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add GitHub Idle Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHIdlePullRequest = namedtuple(\n",
    "    'GHIdlePullRequest',\n",
    "    list(GHPullRequest._fields) + ['open_time', 'idle_time']\n",
    ")\n",
    "\n",
    "def get_github_time_tuple(pull_request):\n",
    "    old_tuple = get_github_tuple(pull_request)\n",
    "\n",
    "    created_at = dateparser.parse(pull_request['created_at'])\n",
    "    open_time = now - created_at\n",
    "\n",
    "    updated_at = dateparser.parse(pull_request['updated_at'])\n",
    "    idle_time = now - updated_at\n",
    "    \n",
    "    new_tuple = GHIdlePullRequest(\n",
    "        open_time=open_time,\n",
    "        idle_time=idle_time,\n",
    "        **old_tuple._asdict()\n",
    "    )\n",
    "\n",
    "    return new_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([get_github_time_tuple(pull_request) for pull_request in seen_pull_requests.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add JIRA Idle Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Group By Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_daycount_string(time_delta):\n",
    "    elapsed = float(time_delta.days) + float(time_delta.seconds) / (60 * 60 * 24)\n",
    "    elapsed_string = '%0.1f days' % elapsed\n",
    "\n",
    "    return elapsed_string\n",
    "\n",
    "def report_active(outfile, jira_issues, issues_by_request, active_reviews, seen_pull_requests):\n",
    "    jira_issues_by_key = { issue['key']: issue for issue in jira_issues }\n",
    "\n",
    "    outfile.write('<h2>Active Pull Requests on %s</h2>' % today.isoformat())\n",
    "\n",
    "    outfile.write('<table>')\n",
    "    outfile.write('<tr>')\n",
    "\n",
    "    for header in ['Submitter', 'Pull Request Link', 'Waiting Tickets', 'Open Time', 'Idle Time']:\n",
    "        outfile.write('<th>%s</th>' % header)\n",
    "\n",
    "    outfile.write('</tr>')\n",
    "\n",
    "    for github_url in active_reviews:\n",
    "        outfile.write('<tr>')\n",
    "\n",
    "        if github_url not in seen_pull_requests:\n",
    "            continue\n",
    "        \n",
    "        pull_request = seen_pull_requests[github_url]\n",
    "\n",
    "        # Submitter\n",
    "\n",
    "        outfile.write('<td>%s</td>' % pull_request['user']['login'])\n",
    "\n",
    "        # Pull Request Link\n",
    "\n",
    "        outfile.write('<td><a href=\"%s\">%s#%d</a></td>' % (github_url, pull_request['base']['user']['login'], pull_request['number']))\n",
    "\n",
    "        # Waiting Tickets\n",
    "\n",
    "        affected_issue_keys = [issue_key for issue_key in issues_by_request[github_url]]\n",
    "        affected_issue_urls = ['https://issues.liferay.com/browse/%s' % issue_key for issue_key in affected_issue_keys]\n",
    "        affected_issue_assignees = [jira_issues_by_key[issue_key]['fields']['assignee']['displayName'] for issue_key in affected_issue_keys]\n",
    "\n",
    "        affected_issue_links = [\n",
    "            '<a href=\"%s\">%s</a> (%s)' % (issue_url, issue_key, issue_assignee)\n",
    "                for issue_key, issue_url, issue_assignee in zip(affected_issue_keys, affected_issue_urls, affected_issue_assignees)\n",
    "        ]\n",
    "\n",
    "        outfile.write('<td>%s</td>' % '<br />'.join(affected_issue_links))\n",
    "\n",
    "        # Open Time\n",
    "\n",
    "        created_at = dateparser.parse(pull_request['created_at'])\n",
    "        open_time = now - created_at\n",
    "\n",
    "        outfile.write('<td>%s</td>' % get_daycount_string(open_time))\n",
    "\n",
    "        # Idle Time\n",
    "\n",
    "        updated_at = dateparser.parse(pull_request['updated_at'])\n",
    "        idle_time = now - updated_at\n",
    "\n",
    "        outfile.write('<td>%s</td>' % get_daycount_string(idle_time))\n",
    "\n",
    "        outfile.write('</tr>')\n",
    "\n",
    "    outfile.write('</table>')\n",
    "\n",
    "def report_completed(outfile, jira_issues, issues_by_request, active_reviews, seen_pull_requests):\n",
    "    requests_by_issue = defaultdict(set)\n",
    "\n",
    "    for github_url, issue_keys in issues_by_request.items():\n",
    "        for issue_key in issue_keys:\n",
    "            requests_by_issue[issue_key].add(github_url)\n",
    "\n",
    "    completed_review = []\n",
    "    region_field_name = 'customfield_11523'\n",
    "\n",
    "    for issue in jira_issues:\n",
    "        issue_key = issue['key']\n",
    "        github_urls = requests_by_issue[issue_key]\n",
    "        all_pulls_closed = len([github_url for github_url in github_urls if github_url in active_reviews]) == 0\n",
    "\n",
    "        if not all_pulls_closed:\n",
    "            continue\n",
    "\n",
    "        assignee = issue['fields']['assignee']['displayName']\n",
    "\n",
    "        regions = []\n",
    "\n",
    "        if region_field_name in issue['fields']:\n",
    "            regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "\n",
    "        region = ''\n",
    "\n",
    "        if len(regions) > 0:\n",
    "            region = regions[0]\n",
    "\n",
    "        idle_time = None\n",
    "\n",
    "        for github_url in github_urls:\n",
    "            if github_url not in seen_pull_requests:\n",
    "                continue\n",
    "            \n",
    "            pull_request = seen_pull_requests[github_url]\n",
    "\n",
    "            if pull_request['closed_at'] is None:\n",
    "                continue\n",
    "\n",
    "            closed_at = dateparser.parse(pull_request['closed_at'])\n",
    "            new_idle_time = now - closed_at\n",
    "\n",
    "            if idle_time is None or new_idle_time < idle_time:\n",
    "                idle_time = new_idle_time\n",
    "\n",
    "        completed_review.append((region, issue_key, assignee, idle_time))\n",
    "\n",
    "    completed_review.sort()\n",
    "\n",
    "    if len(completed_review) > 0:\n",
    "        outfile.write('<h2>Review Already Completed for %s</h2>' % today.isoformat())\n",
    "\n",
    "        outfile.write('<table>')\n",
    "        outfile.write('<tr>')\n",
    "\n",
    "        for header in ['Region', 'Ticket', 'Idle Time']:\n",
    "            outfile.write('<th>%s</th>' % header)\n",
    "\n",
    "        for region, issue_key, assignee, idle_time in completed_review:\n",
    "            outfile.write('<tr>')\n",
    "\n",
    "            # Region\n",
    "\n",
    "            outfile.write('<td>%s</td>' % region)\n",
    "\n",
    "            # Ticket\n",
    "\n",
    "            outfile.write('<td><a href=\"https://issues.liferay.com/browse/%s\">%s</a> (%s)</td>' % (issue_key, issue_key, assignee))\n",
    "\n",
    "            # Idle Time\n",
    "\n",
    "            outfile.write('<td>%s</td>' % get_daycount_string(idle_time))\n",
    "\n",
    "            outfile.write('</tr>')\n",
    "\n",
    "        outfile.write('</table>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report_file_name = 'report_%s.html' % today.isoformat()\n",
    "\n",
    "with open(report_file_name, 'w') as outfile:\n",
    "    report_active(outfile, jira_issues, issues_by_request, active_reviews, seen_pull_requests)\n",
    "    report_completed(outfile, jira_issues, issues_by_request, active_reviews, seen_pull_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort-Merge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are concerned about worst case runtimes, you could instead build a tree-like data structure that has $O(\\log n)$ insertion and lookup time. As a result, the hash join would have a worst-case runtime of $O(m \\log n) + O(n \\log n)$.\n",
    "\n",
    "Imagine that for every pull request, we've extracted its pull request URL and created a sorted data structure that allows us to lookup pull request metadata by its URL. Imagine also that we've extracted all the pull requests from every LPP ticket and we have a sorted data structure that allows us to lookup every JIRA issue corresponding to each GitHub URL.\n",
    "\n",
    "A naive solution that uses these two data structures is a hash join. A smarter solution is to view this as the equivalent of merging two already sorted lists via merge sort.\n",
    "\n",
    "* [Sorting visualizations](http://cs.stanford.edu/people/jcjohns/sorting.js/)\n",
    "\n",
    "A sort-merge join is a join where the query optimizer decides the best way to accomplish the join is to sort the two tables on the specified join keys and then walk both tables in the same way as the merge step of merge sort.\n",
    "\n",
    "* [Sort-merge join](https://en.wikipedia.org/wiki/Sort-merge_join)\n",
    "\n",
    "Note that it doesn't necessarily have to use merge sort for this sort step, though merge sort is well-known to be one of the best external sorting algorithms (it's used by Hadoop during its shuffle step, for example).\n",
    "\n",
    "* [External sorting](https://en.wikipedia.org/wiki/External_sorting)\n",
    "\n",
    "From there, it will compare the sorted tables. It will place a cursor at the smallest key value for both tables (at the top of the sorted tables). At each step, it determines whether it should advance the cursor on one table or the other based on the values of the keys for the rows located at the current cursor position.\n",
    "\n",
    "If you need a visualization in order to understand how the cursor advances in merge sort (possibly due to lack of familiarity with the merge sort algorithm), you're encouraged to consult this visualization.\n",
    "\n",
    "* https://www.youtube.com/watch?v=kPRA0W1kECg#t=1m7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Notebook to Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will use `jupyter nbconvert` to build an `checklpp.py` which runs the script outlined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var script_file = 'checklpp.py';\n",
    "\n",
    "var notebook_name = window.document.getElementById('notebook_name').innerHTML;\n",
    "var nbconvert_command = 'jupyter nbconvert --stdout --to script ' + notebook_name;\n",
    "\n",
    "var grep_command = \"grep -v '^#' | grep -v -F get_ipython | sed '/^$/N;/^\\\\n$/D'\";\n",
    "var command = '!' + nbconvert_command + ' | ' + grep_command + ' > ' + script_file;\n",
    "\n",
    "if (Jupyter.notebook.kernel) {\n",
    "    Jupyter.notebook.kernel.execute(command);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
