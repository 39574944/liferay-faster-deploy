{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins: Combining JIRA With GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, our research question is as follows:\n",
    "\n",
    "<b style=\"color:crimson\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "In order to answer this question, this notebook introduces database joins by creating a script that combines data retrieved from JIRA with data retrieved from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y numpy pandas pytz requests ujson\n",
    "!pip install dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "import dateparser\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import re\n",
    "import requests\n",
    "import six\n",
    "import subprocess\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we'll make sure that we establish one rule for this script and all future scripts: we will save all raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p rawdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important, because one of the more common things to do as a developer is to retrieve the data, extract only the information you need, and then discard the data you do not need. However, unless you have some terms of service agreement restricting what data you are allowed to retain, do not discard the raw data! Raw data is a starting point that speeds up the creation of many different application prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "now = datetime.now(pytz.utc)\n",
    "\n",
    "def load_raw_data(base_name):\n",
    "    file_name = 'rawdata/%s_%s.json' % (base_name, today.isoformat())\n",
    "\n",
    "    if not os.path.isfile(file_name):\n",
    "        return None\n",
    "\n",
    "    with open(file_name) as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "def save_raw_data(base_name, json_value):\n",
    "    file_name = 'rawdata/%s_%s.json' % (base_name, today.isoformat())\n",
    "\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(json_value, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from JIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can answer the question of whether anything is *stuck in review*, we will need to answer the broader question of what is *in review*. We'll start by looking at what's in review in JIRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to JIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because [issues.liferay.com](https://issues.liferay.com/) does not have OAuth support, we will need to find a different way to connect to our JIRA installation. The simplest way is to simply login to JIRA.\n",
    "\n",
    "There are a lot of secure ways to specify your username and password, but for the sake of this script, we'll use the most insecure way possible: a plain text file. Namely, the `.gitconfig` in your user's home folder. If you want to use a different strategy that's less global (a plain text JSON file in the same folder as this script, for example) or more secure, just change the implementation of the two functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(key):\n",
    "    try:\n",
    "        return subprocess.check_output(['git', 'config', key]).strip().decode('utf8')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def set_config(key, value):\n",
    "    subprocess.call(['git', 'config', '--global', key, value])\n",
    "    subprocess.call(['git', 'config', '--global', key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you are using the default implementation, set your username and password inside of the `.gitconfig` located in your user's home folder by running the following two commands in a command line window.\n",
    "\n",
    "``` .sh\n",
    "git config --global jira.session-username JIRA_USERNAME\n",
    "git config --global jira.session-password JIRA_PASSWORD\n",
    "```\n",
    "\n",
    "The following cell will read it in and confirm that it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jira_username = get_config('jira.session-username')\n",
    "jira_password = get_config('jira.session-password')\n",
    "\n",
    "assert(jira_username is not None)\n",
    "assert(jira_password is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will use this information and attempt to login to JIRA and confirm that it has a valid session, which will confirm that the credentials you saved are valid. It will also save this session information so that it can reuse it later without constantly relogging in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jira_base_url = 'https://issues.liferay.com/rest'\n",
    "\n",
    "def get_jira_cookie():\n",
    "    jira_cookie = None\n",
    "\n",
    "    jira_cookie_name = None\n",
    "    jira_cookie_value = None\n",
    "\n",
    "    try:\n",
    "        jira_cookie_name = get_config('jira.session-cookie-name')\n",
    "        jira_cookie_value = get_config('jira.session-cookie-value')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if jira_cookie_name is not None and jira_cookie_value is not None:\n",
    "        jira_cookie = {\n",
    "            jira_cookie_name: jira_cookie_value\n",
    "        }\n",
    "\n",
    "        r = requests.get(jira_base_url + '/auth/1/session', cookies=jira_cookie)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            jira_cookie = None\n",
    "\n",
    "    if jira_cookie is not None:\n",
    "        return jira_cookie\n",
    "        \n",
    "    post_json = {\n",
    "        'username': jira_username,\n",
    "        'password': jira_password\n",
    "    }\n",
    "\n",
    "    r = requests.post(jira_base_url + '/auth/1/session', json=post_json)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print('Invalid login')\n",
    "\n",
    "        return None\n",
    "\n",
    "    response_json = r.json()\n",
    "\n",
    "    jira_cookie_name = response_json['session']['name']\n",
    "    jira_cookie_value = response_json['session']['value']\n",
    "\n",
    "    set_config('jira.session-cookie-name', jira_cookie_name)\n",
    "    set_config('jira.session-cookie-value', jira_cookie_value)\n",
    "\n",
    "    jira_cookie = {\n",
    "        jira_cookie_name: jira_cookie_value\n",
    "    }\n",
    "\n",
    "    return jira_cookie\n",
    "\n",
    "assert(get_jira_cookie() is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve JIRA Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a valid login, our next step is to use the JIRA API to retrieve tickets. If you've interacted with JIRA before, you know that it has its own query language (JQL). It turns out there is a simple search API that allows you to submit the JQL and all the matching issues are returned as JSON. Since the API is fairly simple, we implement it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jira_issues(jql):\n",
    "    jira_cookie = get_jira_cookie()\n",
    "\n",
    "    if jira_cookie is None:\n",
    "        return []\n",
    "\n",
    "    start_at = 0\n",
    "\n",
    "    post_json = {\n",
    "        'jql': jql,\n",
    "        'startAt': start_at\n",
    "    }\n",
    "\n",
    "    r = requests.post(jira_base_url + '/api/2/search', cookies=jira_cookie, json=post_json)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    response_json = r.json()\n",
    "\n",
    "    issues = response_json['issues']\n",
    "\n",
    "    while start_at + response_json['maxResults'] < response_json['total']:\n",
    "        start_at += response_json['maxResults']\n",
    "        post_json['startAt'] = start_at\n",
    "\n",
    "        r = requests.post(jira_base_url + '/api/2/search', cookies=jira_cookie, json=post_json)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return issues\n",
    "\n",
    "        response_json = r.json()\n",
    "\n",
    "        issues.extend(response_json['issues'])\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have something that can retrieve JIRA issues, all we need is to actually create our JQL and then run the search. This is what that code looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jira_issues = load_raw_data('jira_issues')\n",
    "\n",
    "if jira_issues is None:\n",
    "    print('Executing JIRA search')\n",
    "\n",
    "    jql = 'project = LPP AND type not in (\"SME Request\", \"SME Request SubTask\") AND status = \"In Review\" order by key'\n",
    "    jira_issues = get_jira_issues(jql)\n",
    "    save_raw_data('jira_issues', jira_issues)\n",
    "else:\n",
    "    print('Loaded cached JIRA search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, let's take a look at the data we've retrieved. Looking at JSON is a bit tedious, so we'll take a look at a subset of fields in a way that resembles the view you get when you run JQL via the web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JIRAIssue = namedtuple('JIRAIssue', ['key', 'status', 'assignee', 'summary'])\n",
    "\n",
    "pd.DataFrame([\n",
    "    JIRAIssue(\n",
    "        key=jira_issue['key'],\n",
    "        status=jira_issue['fields']['status']['name'],\n",
    "        assignee=jira_issue['fields']['assignee']['displayName'],\n",
    "        summary=jira_issue['fields']['summary']\n",
    "    )\n",
    "        for jira_issue in jira_issues\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted previously, before we can answer the question of whether anything is *stuck in review*, we will need to answer the broader question of what is *in review*. Now that we know what's in review in JIRA, we also want to what's in review on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, [api.github.com](https://developer.github.com/v3/) does have OAuth support, so we'll want to request an OAuth token from GitHub and leverage it in our script.\n",
    "\n",
    "* [Creating a personal access token for command line](https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/)\n",
    "\n",
    "Assuming you are using the default implementation, set your OAuth token inside of the `.gitconfig` located in your user's home folder by running the following command in a command line window.\n",
    "\n",
    "``` .sh\n",
    "git config --global github.oauth-token GITHUB_OAUTH_TOKEN\n",
    "```\n",
    "\n",
    "The following cell will read it in and confirm that it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "github_oauth_token = get_config('github.oauth-token')\n",
    "\n",
    "assert(github_oauth_token is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's not enough that it exists. We should confirm that the token works on the repositories that are related to Liferay. We'll use the `liferay/liferay-portal` and `liferay/liferay-portal-ee` repositories as a way to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_base_url = 'https://api.github.com'\n",
    "\n",
    "def is_repository_accessible(reviewer_url):\n",
    "    print('Validating OAuth token against %s' % reviewer_url)\n",
    "\n",
    "    headers = {\n",
    "        'user-agent': 'python checklpp.py',\n",
    "        'authorization': 'token %s' % github_oauth_token\n",
    "    }\n",
    "\n",
    "    api_path = '/repos/%s' % reviewer_url\n",
    "    \n",
    "    r = requests.get(github_base_url + api_path, headers=headers)\n",
    "    \n",
    "    return r.status_code == 200\n",
    "\n",
    "assert(is_repository_accessible('liferay/liferay-portal'))\n",
    "assert(is_repository_accessible('liferay/liferay-portal-ee'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve GitHub Pull Requests, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a valid login, our next step is to use the GitHub API to retrieve pull requests.\n",
    "\n",
    "Unfortunately, given the number of unique users repositories that are involved in reviewing Liferay pull requests, it's not practical to make an API call against every user and every repository. Instead, we'll want to do this on-demand based on which pull requests we know exist.\n",
    "\n",
    "If you visit `/repos/USERNAME/REPOSITORY/pulls` without a pull ID, GitHub will return all currently open pull requests. The approach below uses this API in an attempt to reduce the number of API requests to GitHub, because this will fetch all pull requests that are open in a single request.\n",
    "\n",
    "Following our principle of keeping any data we retrieve, we save all of these pull requests, and then we request any additional pull requests that are closed and save those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_pull_requests(reviewer_url, pull_request_ids=[]):\n",
    "    print('Checking pull requests waiting on %s' % reviewer_url)\n",
    "\n",
    "    headers = {\n",
    "        'user-agent': 'python checklpp.py',\n",
    "        'authorization': 'token %s' % github_oauth_token\n",
    "    }\n",
    "\n",
    "    api_path = '/repos/%s/pulls' % reviewer_url\n",
    "\n",
    "    r = requests.get(github_base_url + api_path, headers=headers)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return {}\n",
    "\n",
    "    new_pull_requests = r.json()\n",
    "\n",
    "    new_seen_pull_requests = { pull_request['html_url']: pull_request for pull_request in new_pull_requests }\n",
    "\n",
    "    for pull_request_id in pull_request_ids:\n",
    "        github_url = 'https://github.com/%s/pull/%s' % (reviewer_url, pull_request_id)\n",
    "\n",
    "        if github_url in new_seen_pull_requests:\n",
    "            continue\n",
    "\n",
    "        api_path = '/repos/%s/pulls/%s' % (reviewer_url, pull_request_id)\n",
    "\n",
    "        r = requests.get(github_base_url + api_path, headers=headers)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        new_seen_pull_requests[github_url] = r.json()\n",
    "\n",
    "    return new_seen_pull_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll take a look at all the pull requests open against `liferay/liferay-portal-ee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_backport_pulls = load_raw_data('open_backport_pulls')\n",
    "\n",
    "if open_backport_pulls is None:\n",
    "    open_backport_pulls = retrieve_pull_requests('liferay/liferay-portal-ee')\n",
    "    \n",
    "    save_raw_data('open_backport_pulls', open_backport_pulls)\n",
    "else:\n",
    "    print('Loaded cached open backports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, let's take a look at the data we've retrieved. Again, looking at JSON is a bit tedious, so we'll take a look at a subset of fields as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHPullRequest = namedtuple('GHPullRequest', ['submitter', 'reviewer', 'repository', 'branch', 'github_url'])\n",
    "\n",
    "pd.DataFrame([\n",
    "    GHPullRequest(\n",
    "        submitter=pull_request['user']['login'],\n",
    "        reviewer=pull_request['base']['user']['login'],\n",
    "        repository=pull_request['base']['repo']['name'],\n",
    "        branch=pull_request['base']['ref'],\n",
    "        github_url=github_url\n",
    "    )\n",
    "        for github_url, pull_request in open_backport_pulls.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Join Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two data sets, and our next step is to join them together in order to answer our original question.\n",
    "\n",
    "<b style=\"color:crimson\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Now that we've identified which LPP tickets are currently in review and we have created a function that can retrieve the GitHub pull requests are in review for any given user and repository, we need to now answer the following question: given a list of LPP tickets and given a list of pull requests, how do we get the pull request metadata tied to all LPP tickets?\n",
    "\n",
    "If you learned joins while working in the industry, you might have learned joins when you ran into something similar to this situation, where you needed to derive new information from two separate tables. These two tables would have had two columns that referred to the same abstract concept (or perhaps a third \"mapping table\" that describes that abstract concept).\n",
    "\n",
    "* [Khan Academy: Joining Related Tables](https://www.khanacademy.org/computing/computer-programming/sql/relational-queries-in-sql/p/joining-related-tables)\n",
    "\n",
    "We've identified all of the JIRA issues that are in review. Presumably, we want to then combine this with GitHub data. What algorithm will we implement to achieve this? It turns out there are three basic strategies for computing a join: a nested loop join, a hash join, and a sort-merge join.\n",
    "\n",
    "* [Join methods and subqueries](http://www.orafaq.com/tuningguide/join%20methods.html)\n",
    "\n",
    "I believe sort-merge will make more sense after we're finished than before we've finished, and so we'll look first at how a nested loop join and a hash join relate to our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Loop Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple solution to this problem would be to iterate over each of our pull requests and then check each JIRA ticket to see if the JIRA ticket references the pull request. So for every GitHub pull request, you would check every JIRA ticket. This strategy is known as a **nested loop join**.\n",
    "\n",
    "A nested loop join is a join where the query optimizer decides the best way to accomplish the join is to designate one table as the \"outer table\" (no connection to the notion of an outer join) and the other table as the \"inner table\", structured in much the same way as a for loop.\n",
    "\n",
    "* [Nested loop join](https://en.wikipedia.org/wiki/Nested_loop_join)\n",
    "\n",
    "How fast is a nested loop join? Let $m$ be the size of the JIRA table and $n$ be the size of the GitHub table. Regardless of which table you choose for the outer table, a nested loop join would have an expected runtime and a worst-case runtime of $O(m \\cdot n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to this problem would be to first load all the pull requests into a lookup data structure. From there, iterate over the JIRA tickets and match the pull requests to the hash table. So for every JIRA ticket, you would perform a number of lookups against our data structure not based on either tables size (so, effectively a constant). This strategy is known as a **hash join**.\n",
    "\n",
    "A hash join is a join where the query optimizer decides the best way to accomplish the join is to build a lookup table on the smaller table (because it's more likely that a smaller table can fit into memory), with a popular choice being a hash table, and then iterate over the larger table.\n",
    "\n",
    "* [Hash join](https://en.wikipedia.org/wiki/Hash_join)\n",
    "\n",
    "How fast is a hash join? Let $m$ be the size of the JIRA table and $n$ be the size of the GitHub table. Note that hash tables have an $O(1)$ expected access and insertion time, but an $O(n)$ worst case access and insertion time (due to hash collisions, table resizes). So in practice, we have an expected runtime that is $O(m) + O(n)$ and a worst-case runtime that is $O(m \\cdot n) + O(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine JIRA Data with GitHub Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between a nested loop join and a hash join, the hash join has the better expected runtime, and we already have a lookup data structure containing all the pull requests. So, we'll proceed with implementing the hash join, and we'll perform additional optimizations as we proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Mapping Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to iterate over each JIRA ticket and extract the pull requests contained in the ticket. Once we have that information, we'll be able to use our lookup data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_pull_requests_in_review(jira_issues):\n",
    "    issues_by_request = defaultdict(set)\n",
    "    requests_by_reviewer = defaultdict(set)\n",
    "\n",
    "    for jira_issue in jira_issues:\n",
    "        for value in jira_issue['fields'].values():\n",
    "            if not isinstance(value, six.string_types):\n",
    "                continue\n",
    "\n",
    "            for github_url in re.findall('https://github.com/[^\\s]*/pull/[\\d]+', value):\n",
    "                issues_by_request[github_url].add(jira_issue['key'])\n",
    "\n",
    "                pos = github_url.find('/', github_url.find('/', 19) + 1)\n",
    "                reviewer_url = github_url[19:pos]\n",
    "                pull_request_id = github_url[github_url.rfind('/') + 1:]\n",
    "\n",
    "                requests_by_reviewer[reviewer_url].add(pull_request_id)\n",
    "\n",
    "    return issues_by_request, requests_by_reviewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pass the data the results of our previous JIRA search to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_by_request = load_raw_data('issues_by_request')\n",
    "requests_by_reviewer = load_raw_data('requests_by_reviewer')\n",
    "\n",
    "if issues_by_request is None or requests_by_reviewer is None:\n",
    "    issues_by_request, requests_by_reviewer = extract_pull_requests_in_review(jira_issues)\n",
    "\n",
    "    save_raw_data('issues_by_request', issues_by_request)\n",
    "    save_raw_data('requests_by_reviewer', requests_by_reviewer)\n",
    "else:\n",
    "    print('Loaded cached JIRA to GitHub mapping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we were to represent this data as a table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JIRAGitHubMapping = namedtuple('JIRAGitHubMapping', ['jiraKey', 'githubKey'])\n",
    "\n",
    "pd.DataFrame([\n",
    "    JIRAGitHubMapping(jiraKey=jiraKey, githubKey=githubKey)\n",
    "        for githubKey, jiraKeys in issues_by_request.items()\n",
    "            for jiraKey in jiraKeys\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it turns out that what we've done is equivalent to building a mapping table between the JIRA tickets (represented with their issue key) and the GitHub pull requests (represented with their URL).\n",
    "\n",
    "* [Understanding Mapping Tables](https://stackoverflow.com/questions/6453462/mysql-understanding-mapping-tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve GitHub Pull Requests, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll want to fetch all the metadata associated with all those pull requests. We'll also want a quick way to separate the open/active pull requests from the closed/inactive pull requests so that we don't have to re-derive that metadata later on when we attempt to identify stuck pull requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_active_pull_request_reviews(issues_by_request, requests_by_reviewer):\n",
    "    active_reviews = []\n",
    "    seen_pull_requests = {}\n",
    "\n",
    "    for reviewer_url, pull_request_ids in sorted(requests_by_reviewer.items()):\n",
    "        new_seen_pull_requests = retrieve_pull_requests(reviewer_url, pull_request_ids)\n",
    "        new_active_reviews = [pull_request['html_url'] for pull_request in new_seen_pull_requests.values() if pull_request['html_url'] in issues_by_request and pull_request['state'] != 'closed']\n",
    "\n",
    "        seen_pull_requests.update(new_seen_pull_requests)\n",
    "        active_reviews.extend(new_active_reviews)\n",
    "\n",
    "    return active_reviews, seen_pull_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and use our mapping tables to populate our GitHub table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_reviews = load_raw_data('active_reviews')\n",
    "seen_pull_requests = load_raw_data('seen_pull_requests')\n",
    "\n",
    "if active_reviews is None or seen_pull_requests is None:\n",
    "    active_reviews, seen_pull_requests = retrieve_active_pull_request_reviews(issues_by_request, requests_by_reviewer)\n",
    "\n",
    "    save_raw_data('active_reviews', active_reviews)\n",
    "    save_raw_data('seen_pull_requests', seen_pull_requests)\n",
    "else:\n",
    "    print('Loaded cached pull request metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_daycount_string(time_delta):\n",
    "    elapsed = float(time_delta.days) + float(time_delta.seconds) / (60 * 60 * 24)\n",
    "    elapsed_string = '%0.1f days' % elapsed\n",
    "\n",
    "    return elapsed_string\n",
    "\n",
    "def report_active(outfile, jira_issues, issues_by_request, active_reviews, seen_pull_requests):\n",
    "    jira_issues_by_key = { issue['key']: issue for issue in jira_issues }\n",
    "\n",
    "    outfile.write('<h2>Active Pull Requests on %s</h2>' % today.isoformat())\n",
    "\n",
    "    outfile.write('<table>')\n",
    "    outfile.write('<tr>')\n",
    "\n",
    "    for header in ['Submitter', 'Pull Request Link', 'Waiting Tickets', 'Open Time', 'Idle Time']:\n",
    "        outfile.write('<th>%s</th>' % header)\n",
    "\n",
    "    outfile.write('</tr>')\n",
    "\n",
    "    for github_url in active_reviews:\n",
    "        outfile.write('<tr>')\n",
    "\n",
    "        if github_url not in seen_pull_requests:\n",
    "            continue\n",
    "        \n",
    "        pull_request = seen_pull_requests[github_url]\n",
    "\n",
    "        # Submitter\n",
    "\n",
    "        outfile.write('<td>%s</td>' % pull_request['user']['login'])\n",
    "\n",
    "        # Pull Request Link\n",
    "\n",
    "        outfile.write('<td><a href=\"%s\">%s#%d</a></td>' % (github_url, pull_request['base']['user']['login'], pull_request['number']))\n",
    "\n",
    "        # Waiting Tickets\n",
    "\n",
    "        affected_issue_keys = [issue_key for issue_key in issues_by_request[github_url]]\n",
    "        affected_issue_urls = ['https://issues.liferay.com/browse/%s' % issue_key for issue_key in affected_issue_keys]\n",
    "        affected_issue_assignees = [jira_issues_by_key[issue_key]['fields']['assignee']['displayName'] for issue_key in affected_issue_keys]\n",
    "\n",
    "        affected_issue_links = [\n",
    "            '<a href=\"%s\">%s</a> (%s)' % (issue_url, issue_key, issue_assignee)\n",
    "                for issue_key, issue_url, issue_assignee in zip(affected_issue_keys, affected_issue_urls, affected_issue_assignees)\n",
    "        ]\n",
    "\n",
    "        outfile.write('<td>%s</td>' % '<br />'.join(affected_issue_links))\n",
    "\n",
    "        # Open Time\n",
    "\n",
    "        created_at = dateparser.parse(pull_request['created_at'])\n",
    "        open_time = now - created_at\n",
    "\n",
    "        outfile.write('<td>%s</td>' % get_daycount_string(open_time))\n",
    "\n",
    "        # Idle Time\n",
    "\n",
    "        updated_at = dateparser.parse(pull_request['updated_at'])\n",
    "        idle_time = now - updated_at\n",
    "\n",
    "        outfile.write('<td>%s</td>' % get_daycount_string(idle_time))\n",
    "\n",
    "        outfile.write('</tr>')\n",
    "\n",
    "    outfile.write('</table>')\n",
    "\n",
    "def report_completed(outfile, jira_issues, issues_by_request, active_reviews, seen_pull_requests):\n",
    "    requests_by_issue = defaultdict(set)\n",
    "\n",
    "    for github_url, issue_keys in issues_by_request.items():\n",
    "        for issue_key in issue_keys:\n",
    "            requests_by_issue[issue_key].add(github_url)\n",
    "\n",
    "    completed_review = []\n",
    "    region_field_name = 'customfield_11523'\n",
    "\n",
    "    for issue in jira_issues:\n",
    "        issue_key = issue['key']\n",
    "        github_urls = requests_by_issue[issue_key]\n",
    "        all_pulls_closed = len([github_url for github_url in github_urls if github_url in active_reviews]) == 0\n",
    "\n",
    "        if not all_pulls_closed:\n",
    "            continue\n",
    "\n",
    "        assignee = issue['fields']['assignee']['displayName']\n",
    "\n",
    "        regions = []\n",
    "\n",
    "        if region_field_name in issue['fields']:\n",
    "            regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "\n",
    "        region = ''\n",
    "\n",
    "        if len(regions) > 0:\n",
    "            region = regions[0]\n",
    "\n",
    "        idle_time = None\n",
    "\n",
    "        for github_url in github_urls:\n",
    "            if github_url not in seen_pull_requests:\n",
    "                continue\n",
    "            \n",
    "            pull_request = seen_pull_requests[github_url]\n",
    "\n",
    "            if pull_request['closed_at'] is None:\n",
    "                continue\n",
    "\n",
    "            closed_at = dateparser.parse(pull_request['closed_at'])\n",
    "            new_idle_time = now - closed_at\n",
    "\n",
    "            if idle_time is None or new_idle_time < idle_time:\n",
    "                idle_time = new_idle_time\n",
    "\n",
    "        completed_review.append((region, issue_key, assignee, idle_time))\n",
    "\n",
    "    completed_review.sort()\n",
    "\n",
    "    if len(completed_review) > 0:\n",
    "        outfile.write('<h2>Review Already Completed for %s</h2>' % today.isoformat())\n",
    "\n",
    "        outfile.write('<table>')\n",
    "        outfile.write('<tr>')\n",
    "\n",
    "        for header in ['Region', 'Ticket', 'Idle Time']:\n",
    "            outfile.write('<th>%s</th>' % header)\n",
    "\n",
    "        for region, issue_key, assignee, idle_time in completed_review:\n",
    "            outfile.write('<tr>')\n",
    "\n",
    "            # Region\n",
    "\n",
    "            outfile.write('<td>%s</td>' % region)\n",
    "\n",
    "            # Ticket\n",
    "\n",
    "            outfile.write('<td><a href=\"https://issues.liferay.com/browse/%s\">%s</a> (%s)</td>' % (issue_key, issue_key, assignee))\n",
    "\n",
    "            # Idle Time\n",
    "\n",
    "            outfile.write('<td>%s</td>' % get_daycount_string(idle_time))\n",
    "\n",
    "            outfile.write('</tr>')\n",
    "\n",
    "        outfile.write('</table>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report_file_name = 'report_%s.html' % today.isoformat()\n",
    "\n",
    "with open(report_file_name, 'w') as outfile:\n",
    "    report_active(outfile, jira_issues, issues_by_request, active_reviews, seen_pull_requests)\n",
    "    report_completed(outfile, jira_issues, issues_by_request, active_reviews, seen_pull_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort-Merge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are concerned about worst case runtimes, you could instead build a tree-like data structure that has $O(\\log n)$ insertion and lookup time. As a result, the hash join would have a worst-case runtime of $O(m \\log n) + O(n \\log n)$.\n",
    "\n",
    "Imagine that for every pull request, we've extracted its pull request URL and created a sorted data structure that allows us to lookup pull request metadata by its URL. Imagine also that we've extracted all the pull requests from every LPP ticket and we have a sorted data structure that allows us to lookup every JIRA issue corresponding to each GitHub URL.\n",
    "\n",
    "A naive solution that uses these two data structures is a hash join. A smarter solution is to view this as the equivalent of merging two already sorted lists via merge sort.\n",
    "\n",
    "* [Sorting visualizations](http://cs.stanford.edu/people/jcjohns/sorting.js/)\n",
    "\n",
    "A sort-merge join is a join where the query optimizer decides the best way to accomplish the join is to sort the two tables on the specified join keys and then walk both tables in the same way as the merge step of merge sort.\n",
    "\n",
    "* [Sort-merge join](https://en.wikipedia.org/wiki/Sort-merge_join)\n",
    "\n",
    "Note that it doesn't necessarily have to use merge sort for this sort step, though merge sort is well-known to be one of the best external sorting algorithms (it's used by Hadoop during its shuffle step, for example).\n",
    "\n",
    "* [External sorting](https://en.wikipedia.org/wiki/External_sorting)\n",
    "\n",
    "From there, it will compare the sorted tables. It will place a cursor at the smallest key value for both tables (at the top of the sorted tables). At each step, it determines whether it should advance the cursor on one table or the other based on the values of the keys for the rows located at the current cursor position.\n",
    "\n",
    "If you need a visualization in order to understand how the cursor advances in merge sort (possibly due to lack of familiarity with the merge sort algorithm), you're encouraged to consult this visualization.\n",
    "\n",
    "* https://www.youtube.com/watch?v=kPRA0W1kECg#t=1m7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will happen from here is you ask the database to provide an explanation of what it's doing for your query, you work some magic to make this explanation look better (add database indices, modify the query), and you commit those changes to the codebase.\n",
    "\n",
    "* [Query plan](https://en.wikipedia.org/wiki/Query_plan)\n",
    "\n",
    "Our definition of \"better\" comes from interpreting certain aspects of the query plan as necessarily worse than what we expected. Our expectations often come from definitions that are provided by various database vendors on the ideal query plan.\n",
    "\n",
    "* [MySQL explain plan](https://dev.mysql.com/doc/refman/5.6/en/explain-output.html)\n",
    "* [Oracle explain plan](https://docs.oracle.com/database/121/TGSQL/tgsql_interp.htm)\n",
    "* [PostgreSQL explain plan](http://www.postgresql.org/docs/9.4/static/using-explain.html)\n",
    "* [SQL Server explain plan](https://technet.microsoft.com/en-us/library/ms178071%28v=sql.105%29.aspx)\n",
    "\n",
    "With that in mind, let's talk about implementation details (rather than relying solely on high level intuitions) in order to enhance your knowledge about why certain joins are better than others and why the query plan may choose certain approaches whenever it is confronted with your query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Notebook to Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will use `jupyter nbconvert` to build an `checklpp.py` which runs the script outlined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var script_file = 'checklpp.py';\n",
    "\n",
    "var notebook_name = window.document.getElementById('notebook_name').innerHTML;\n",
    "var nbconvert_command = 'jupyter nbconvert --stdout --to script ' + notebook_name;\n",
    "\n",
    "var grep_command = \"grep -v '^#' | grep -v -F get_ipython | sed '/^$/N;/^\\\\n$/D'\";\n",
    "var command = '!' + nbconvert_command + ' | ' + grep_command + ' > ' + script_file;\n",
    "\n",
    "if (Jupyter.notebook.kernel) {\n",
    "    Jupyter.notebook.kernel.execute(command);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
