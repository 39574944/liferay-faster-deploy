{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 3: Combining LESA with JIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we start with the following research question. \"Can we create data visualizations on top of the LESA, LPS, LPP, and BPR ticket metadata that lets us group together different tickets so that we can explore the times that tickets remain in each status based on those groupings?\"\n",
    "\n",
    "While this is inherently a data visualization question, it has one prerequisite question that has not yet been answered: \"Can we bring together LESA, LPS, LPP, and BPR ticket metadata?\"\n",
    "\n",
    "However, this question still fairly ambitious for a tutorial that intends to introduce you to joining LESA data with JIRA data, as it requires aggregating all of it. For now, we'll look at this reduced scope.\n",
    "\n",
    "<b style=\"color:green\">Can we bring together LESA, LPS, LPP, and BPR ticket metadata for DXP?</b>\n",
    "\n",
    "This notebook assumes that you've loaded a backup of the LESA database from `files.liferay.com` into a database named `lportal`, because LESA (like many internal Liferay systems) lacks a useful API for data analysis, and therefore we will extract the data by querying the database directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y beautifulsoup4 mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from checklpp import *\n",
    "\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we'll make sure that we establish one rule for this script and all future scripts: we will save all raw data with timestamps. This time, we're going to be saving the results of a database query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    user='lportal', password=get_config('mysql.password'),\n",
    "    host='ec2-34-208-59-105.us-west-2.compute.amazonaws.com', database='lportal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_query(cache_name, query, row_function=None):\n",
    "    file_name = get_file_name(cache_name, 'json')\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        print('[%s] Skipping query execution due to cache file' % datetime.now().isoformat())\n",
    "        return\n",
    "\n",
    "    print('[%s] Executing query %s' % (datetime.now().isoformat(), query))\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        for i, item in enumerate(cursor):\n",
    "            if i % 1000 == 0:\n",
    "                print('[%s] Processed %d items' % (datetime.now().isoformat(), i))\n",
    "\n",
    "            row_value = {key: value for key, value in zip(cursor.column_names, item)}\n",
    "\n",
    "            if row_function is None:\n",
    "                save_row(outfile, [], row_value)\n",
    "                continue\n",
    "\n",
    "            for return_value in row_function(row_value):\n",
    "                save_row(outfile, [], return_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_query(cache_name, row_function=None):\n",
    "    file_name = get_file_name(cache_name, 'json')\n",
    "\n",
    "    with open(file_name, 'r') as infile:\n",
    "        if row_function is None:\n",
    "            return [load_row(line) for line in infile]\n",
    "        else:\n",
    "            return [row_function(load_row(line)) for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explain_query(query):\n",
    "    print('[%s] Explaining query %s' % (datetime.now().isoformat(), query))\n",
    "\n",
    "    cursor.execute('explain %s' % query)\n",
    "\n",
    "    return [\n",
    "        { key: value for key, value in zip(cursor.column_names, item) }\n",
    "            for item in cursor\n",
    "    ]\n",
    "\n",
    "def run_query(query):\n",
    "    print('[%s] Executing query %s' % (datetime.now().isoformat(), query))\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    return [\n",
    "        { key: value for key, value in zip(cursor.column_names, item) }\n",
    "            for item in cursor\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract JIRA Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Explicit JIRA Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some links are found in the `OSB_TicketLink` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = re.compile('[A-Z]*-[\\d]*')\n",
    "\n",
    "def extract_ticket(row_value):\n",
    "    jira_url = row_value['url']\n",
    "\n",
    "    if jira_url.find('https://issues.liferay.com/browse/') != 0:\n",
    "        yield row_value\n",
    "        return\n",
    "\n",
    "    candidate_key = jira_url[len('https://issues.liferay.com/browse/'):]\n",
    "\n",
    "    result = p1.search(candidate_key)\n",
    "\n",
    "    if result is None:\n",
    "        yield row_value\n",
    "        return\n",
    "\n",
    "    row_value['jira_key'] = result.group(0)\n",
    "    yield row_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from OSB_TicketLink where url like 'https://issues.liferay.com/%'\n",
    "\"\"\"\n",
    "\n",
    "save_query('lesa/JIRALink_1', query, extract_ticket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract JIRA Links in Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some links are buried inside of the Liferay-only sections of comments and never formally linked on the ticket. Therefore, we'll need to perform some text extraction in order to identify those links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p2 = re.compile('https://issues.liferay.com/browse/[A-Z]*-[\\d]*')\n",
    "\n",
    "patterns = [p2]\n",
    "\n",
    "def extract_links(row_value):\n",
    "    for url in [item for p in patterns for item in p.findall(row_value['body'])]:\n",
    "        link_value = {\n",
    "            'userName': row_value['userName'],\n",
    "            'url': url,\n",
    "            'createDate': row_value['createDate'],\n",
    "            'userId': row_value['userId'],\n",
    "            'visibility': row_value['visibility'],\n",
    "            'type_': row_value['type_'],\n",
    "            'ticketEntryId': row_value['ticketEntryId'],\n",
    "            'ticketCommentId': row_value['ticketCommentId']\n",
    "        }\n",
    "\n",
    "        yield extract_ticket(link_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from OSB_TicketComment where body like '%https://issues.liferay.com/%'\n",
    "\"\"\"\n",
    "\n",
    "save_query('lesa/JIRALink_2', query, extract_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Database Indices, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A database index is built around the notion that you might be able to pre-create a lookup structure that the database can use whenever it needs to lookup values, whether that lookup is for a join or that lookup is the actual query.\n",
    "\n",
    "* [Database index](https://en.wikipedia.org/wiki/Database_index)\n",
    "\n",
    "Looking back at what we've already learned about database indices, you can think of a database index as an in-between compromise. Namely, it's a disk-based lookup structure that allows it to favor a different sort of join strategy than a nested loop join. This disk-based structure, if small, can also be loaded to quickly provide a hash-join strategy.\n",
    "\n",
    "So how do database indices achieve this? At a high level, they do it by implementing a column-based lookup structure on top of the row-oriented storage system.\n",
    "\n",
    "At a lower level, there are a variety of these column-based lookup data structures that have performance implications (ones that are probabilistic rather than strictly error-free, ones that require clustering or reordering the rows to improve sequential scans, etc.), but to start out, we'll look at two of the more popular types: b-trees and bitmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [b-tree index](https://en.wikipedia.org/wiki/B-tree) can be conceptualized a disk access efficient implementation of a binary tree, where the keys used for navigating the binary tree are the keys of the index. Because it is a binary tree, it is a sorted data structure.\n",
    "\n",
    "Because the data structure is sorted, this means that it is extremely efficient to walk the keys of the index in sorted order, which will be relevant when we talk about sort-merge joins and range queries.\n",
    "\n",
    "Knowing that the data structure is sorted, this means that the order of the columns for the index matters. For example, if you have one index that indexes the values three columns and you specify two of those columns, the database can only predictably use the database index if the two columns are the first two columns specified for the index.\n",
    "\n",
    "Since the data structure is sorted, this has a specific implication for `like` clauses. Namely, the `%` wildcard will only predictably use the index if it appears at the end, because placing one at the beginning or in the middle will force the database to second guess whether the index will really help query runtime (it usually depends on cardinality and how large the index is relative to the table).\n",
    "\n",
    "To put this into perspective common side-effect of this is that you have to be careful when you build queries for the Liferay `Group_` table. If there is an index on `companyId`, `classNameId`, and `classPK`, a database query that skips the `classNameId` may not be able to use the index. Additionally, Liferay's obsession with alphabetization can theoretically result in unusable indices if we do not pay attention to what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [bitmap index](https://en.wikipedia.org/wiki/Bitmap_index) can be conceptualized as transforming a query into bitwise operations. It achieves this by converting every value for a column into a bitmap or bit set, where each bit corresponds to a row, and the value of the bit (whether it's 0 or 1) corresponds to whether that row has the specified value for this column.\n",
    "\n",
    "This means that for multiple checks against the same table, you simply load the bitmap for each checked column value into vectors of 32-bit or 64-bit blocks, and perform vectorized bitwise operations. The resulting vector precisely describes the rows that satisfy all checks.\n",
    "\n",
    "Naturally, because you can only have zeroes and ones in a bitmap, these can be compressed with run-length encodings to make these space efficient, but run-length encodings and similar compression strategies make it potentially expensive to update the value for a single entry.\n",
    "\n",
    "* [Run-length encoding](https://en.wikipedia.org/wiki/Run-length_encoding)\n",
    "\n",
    "As a result, bitmap indices are uncommon in write-heavy workload tables in active databases, but they are very popular in read-heavy workload tables in data warehouses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Liferay Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each LESA ticket found in the `OSB_TicketEntry` table, we identify the version of Liferay based on the value in the `envLFR` column. Each value in this column corresponds to an entry in the `ListType` table, with a few different types, all of them having a `type_` that starts with `com.liferay.osb.model.ProductEntry`.\n",
    "\n",
    "Of course, not all of these are used.\n",
    "\n",
    "First, we'll find the used `ListType` values that correspond to Liferay versions, which can be achieved by looking for the distinct values of the `envLFR` table and checking for the matching `listTypeId` value in the `ListType` table. The query might look like the following.\n",
    "\n",
    "```\n",
    "SELECT *\n",
    "FROM   ListType\n",
    "WHERE  listTypeId IN (SELECT DISTINCT envLFR\n",
    "                      FROM   OSB_TicketEntry)\n",
    "       AND type_ LIKE 'com.liferay.osb.model.ProductEntry.%'\n",
    "```\n",
    "\n",
    "However, before we do that, we'll need to see if it's a good idea to execute the query by looking at the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guess the Query Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the database indices and compare them to our database query to see if we have an intuition about how the database query will take shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(run_query('show indexes from ListType'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(run_query('select count(*) from ListType'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we see above, there is only one index on the `ListType` table, and the current index statistics estimate there are 97 distinct values in 488 rows. There is an index on the `type_` column, which we've included in our query, so we'd expect the database to be able to use an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(run_query('show indexes from OSB_TicketEntry'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(run_query('select count(*) from OSB_TicketEntry'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that might happen with the above query is that it may demonstrate how the index statistics are approximate, if the database estimates that the cardinality of the `ticketEntryId` column is different from the actual count of entries in the `OSB_TicketEntry` table.\n",
    "\n",
    "From what we see above, there are five different indices, but none of them are on the `envLFR` column. Therefore, we expect for the database to iterate over all of the rows in order to extract the column value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Query Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM   ListType\n",
    "WHERE  listTypeId IN (SELECT DISTINCT envLFR\n",
    "                      FROM   OSB_TicketEntry)\n",
    "       AND type_ LIKE 'com.liferay.osb.model.ProductEntry.%'\n",
    "\"\"\"\n",
    "\n",
    "pd.DataFrame(explain_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've never seen this format before, the MySQL documentation provides a good high-level explanation of what is going on in the above query.\n",
    "\n",
    "* [Explain Output Format](https://dev.mysql.com/doc/refman/5.7/en/explain-output.html#explain-join-types)\n",
    "\n",
    "According to the explain plan, the database will materialize our subquery as a table (which, you might remember from the previous tutorial, is the result of any query).\n",
    "\n",
    "`select envLFR from OSB_TicketEntry`\n",
    "\n",
    "From there, it will compute the distinct values. Most databases compute a distinct by performing a sort and then iterating over the sorted result so that it can pick off the distinct values.\n",
    "\n",
    "This list of distinct values will then be used with a filtered `ListType` table to find the entries of interest. Because we have only a prefix on our `LIKE` clause, and because there is an index on the `type_` column, this filtering is effectively a range query, where everything is greater than our prefix, and everything is less than whatever comes after it in a sorted list of possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pd.DataFrame(run_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we see that our `name` column has a value resembling 7.0 only once, and therefore LESA tickets for DXP correspond to LESA tickets that specify that their `envLFR` value is 41000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from LESA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from OSB_TicketEntry where envLFR = 41000\n",
    "\"\"\"\n",
    "\n",
    "save_query('lesa/OSB_TicketEntry', query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Sort-Merge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look back to hash-joins. For a hash table, in practice, we have an expected runtime that is $O(m) + O(n)$ and a worst-case runtime that is $O(m \\cdot n) + O(n^2)$.\n",
    "\n",
    "If you are concerned about worst case runtimes, you could instead build a tree-like data structure that has $O(\\log n)$ insertion and lookup time. As a result, attempting a hash join using this data structure would have both an expected and a worst-case runtime of $O(m \\log n) + O(n \\log n)$.\n",
    "\n",
    "Now let's take a look at the data we currently have. We have two different data sets that store every link, along with a `ticketEntryId` indicating which ticket contains this link. We also have a data set that stores each ticket coming from DXP, and it also has a `ticketEntryId` column.\n",
    "\n",
    "An initial question we might ask is, given just these two tables, how do we combine these two data sets together? The straightforward solution is to recognize that we could perform a hash join or even a nested loop join, given that these are both very small tables.\n",
    "\n",
    "However, if the tables were both very large, could we theoretically do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort-Merge Clustered Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that all the rows in all our tables were sorted by `ticketEntryId`. In this situation, what we've effectively done is made `ticketEntryId` a clustered index.\n",
    "\n",
    "* [What is a clustered and non-clustered index?](https://www.quora.com/What-is-a-clustered-and-non-clustered-index)\n",
    "\n",
    "In this scenario, you could view the join as the equivalent of merging two already sorted lists via merge sort.\n",
    "\n",
    "* [Sorting visualizations](http://cs.stanford.edu/people/jcjohns/sorting.js/)\n",
    "\n",
    "A sort-merge join is a join where the query optimizer decides the best way to accomplish the join is to sort the two tables on the specified join keys and then walk both tables in the same way as the merge step of merge sort.\n",
    "\n",
    "* [Sort-merge join](https://en.wikipedia.org/wiki/Sort-merge_join)\n",
    "\n",
    "Note that it doesn't necessarily have to use merge sort for this sort step, though merge sort is well-known to be one of the best external sorting algorithms (it's used by Hadoop during its shuffle step, for example).\n",
    "\n",
    "* [External sorting](https://en.wikipedia.org/wiki/External_sorting)\n",
    "\n",
    "From there, it will compare the sorted tables. It will place a cursor at the smallest key value for both tables (at the top of the sorted tables). At each step, it determines whether it should advance the cursor on one table or the other based on the values of the keys for the rows located at the current cursor position.\n",
    "\n",
    "If you need a visualization in order to understand how the cursor advances in merge sort (possibly due to lack of familiarity with the merge sort algorithm), you're encouraged to consult this visualization.\n",
    "\n",
    "* https://www.youtube.com/watch?v=kPRA0W1kECg#t=1m7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort-Merge Sargable Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that both of the tables had a b-tree index on `ticketEntryId`. You would be able to identify which rows in each table needed to be join by walking the b-tree index for both tables in the same merge sort style as for walking two sorted tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
